{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for root_dir and set working directory\n",
      "Working directory set to: /mnt/ssd/git/3d-mri-domain-adaption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Creating directory /mnt/ssd/git/3d-mri-domain-adaption/logs/extract_segments\n",
      "2020-10-13 08:51:44,493 INFO -------------------- Start --------------------\n",
      "2020-10-13 08:51:44,493 INFO Working directory: /mnt/ssd/git/3d-mri-domain-adaption.\n",
      "2020-10-13 08:51:44,493 INFO Log file: ./logs/extract_segments/heiner_test.log\n",
      "2020-10-13 08:51:44,493 INFO Log level for console: INFO\n"
     ]
    }
   ],
   "source": [
    "# define logging and working directory\n",
    "from ProjectRoot import change_wd_to_project_root\n",
    "change_wd_to_project_root()\n",
    "\n",
    "from src.data.Extract_segments_v3 import create_volumes, Console_and_file_logger\n",
    "import logging, os, glob\n",
    "\n",
    "Console_and_file_logger('extract_segments/heiner_test', logging.INFO)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to check for typos or case sensitive differences in xml- and folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 08:52:44,522 INFO no dicom path given, expecting a dicom folder within the xml folder.\n",
      "2020-10-13 08:52:44,523 INFO Found 1 xml-files.\n",
      "2020-10-13 08:52:44,524 INFO Matched: 1 xml files to a dicom folder\n",
      "2020-10-13 08:52:44,525 INFO Didn't find a dicom folder for: 0 xml files\n",
      "2020-10-13 08:52:44,559 INFO --------------------------------------------------\n",
      "2020-10-13 08:52:44,561 INFO processing patient: 1 (SAX_REXUUV3F200511121251461.3.46.670589.11.0.0.11.4.2.0.5167.5.2984.2005111212514448176) of total: 1\n",
      "2020-10-13 08:52:45,321 INFO Image size: (256, 256, 1, 18)\n",
      "2020-10-13 08:52:45,322 INFO Image Spacing: (1.3671875, 1.3671875, 8.80000019073486, 1)\n",
      "2020-10-13 08:52:45,330 INFO process with patient 1 finished after 0.771 sec.\n",
      "2020-10-13 08:52:45,338 INFO All 1 patients done in 0.813 s\n"
     ]
    }
   ],
   "source": [
    "# Extract the masks and images from circle cvi42wsx export files\n",
    "# works simultanous on all CPUs, adjust to your machine\n",
    "\n",
    "# reads all xml file names\n",
    "# reads all dicom file names\n",
    "# maps xml and dicom images according to patient UID and date\n",
    "# extracts the ll contour, contour types and series IDs from the xml files\n",
    "# opens all dicoms with the given series-IDs from the mapped patient folder (we also need the dicom images between the labeled timesteps)\n",
    "# sort/stack dicom images according to the Triggertime and Origin\n",
    "# create a masked twin with the same shape but with the contours\n",
    "# transform the contours to categorical masks (0 = Background, 1 = RV, 2 = Myocard, 3 = LV)\n",
    "# copy all metadata from the original dicom images, + minor encoding cleaning\n",
    "# builds three 4d volumes (MRI image, Mask, mixed Image)\n",
    "# save 4d volumes as nrrd files to given export path\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "# path to the folder, which contains the xml files\n",
    "# path_to_xml = os.path.join(working_dir, '/mnt/data/datasets/cardio/new_gcn_extract/')\n",
    "# path_to_xml = os.path.join(working_dir, 'data/interim/peters_cohort/')\n",
    "path_to_xml = '/mnt/ssd/data/heiner_test_export/'\n",
    "\n",
    "# Define the path to the dicom files\n",
    "path_to_dicom = None # if None --> script expects the dicoms to be in a subfolder within the xml files\n",
    "#path_to_dicom = '/mnt/data/datasets/cardio/ahf_export/'\n",
    "\n",
    "# Define a folder where we can save the nrrd files to\n",
    "path_to_export = os.path.join(working_dir, 'data/raw/heiner_test')\n",
    "\n",
    "create_volumes(path_to_xml, path_to_dicom, path_to_export, max_workers=12, spawn_processes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore the following cells if you dont want to parse manually !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for the long axis contours and the manual labels - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/temp/export/0PTV75MP200506271208231/SAX_0PTV75MP200506271208231.3.46.670589.11.0.0.11.4.2.0.5167.5.5240.2005062712082160343.cvi42wsx',\n",
       " 'data/temp/export/0PTV75MP200506271208231/LAX_0PTV75MP200506271208231.3.46.670589.11.0.0.11.4.2.0.5167.5.5240.2005062712082160343.cvi42wsx',\n",
       " 'data/temp/export/0HQQW4ZN200705231338441/LAX_0HQQW4ZN200705231338441.2.124.113532.192.168.202.14.20070523.131344.1689214.cvi42wsx',\n",
       " 'data/temp/export/0HQQW4ZN200705231338441/SAX_0HQQW4ZN200705231338441.2.124.113532.192.168.202.14.20070523.131344.1689214.cvi42wsx',\n",
       " 'data/temp/export/0AE4R74L190001011238291/SAX_0AE4R74L190001011238291.3.12.2.1107.5.99.2.1013.30000008030612131778100072179.cvi42wsx',\n",
       " 'data/temp/export/0AE4R74L190001011238291/LAX_0AE4R74L190001011238291.3.12.2.1107.5.99.2.1013.30000008030612131778100072179.cvi42wsx',\n",
       " 'data/temp/export/04NEJQUZ200703130733401/LAX_04NEJQUZ200703130733401.3.46.670589.11.0.0.11.4.2.0.8365.5.7500.2007031307333781538.cvi42wsx',\n",
       " 'data/temp/export/04NEJQUZ200703130733401/SAX_04NEJQUZ_200703130733401.3.46.670589.11.0.0.11.4.2.0.8365.5.7500.2007031307333781538.cvi42wsx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all xml files\n",
    "import glob\n",
    "xmls = glob.glob(\"data/temp/export/**/*.cvi42wsx\")\n",
    "xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ = 'data/temp/data/04NEJQUZ_200703130733401/LAX_04NEJQUZ200703130733401.3.46.670589.11.0.0.11.4.2.0.8365.5.7500.2007031307333781538.cvi42wsx'\n",
    "from bs4 import BeautifulSoup\n",
    "# open xml file and create soup\n",
    "with open(f_) as fp:\n",
    "    soup = BeautifulSoup(fp, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = list(soup.find_all('hash:item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = list()\n",
    "for item in all_items:\n",
    "    tag = item.attrs.get('hash:key', False)\n",
    "    if tag:\n",
    "        if '.' not in tag:\n",
    "            cs.append(item.attrs.get('hash:key', False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'lalaContour' in set(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-14 21:49:52,528 INFO -------------------- Start --------------------\n",
      "2019-11-14 21:49:52,529 INFO Working directory: /mnt/data/git/cardio.\n",
      "2019-11-14 21:49:52,529 INFO Log file: ./logs/extract_segmentations_new_fast.log\n",
      "2019-11-14 21:49:52,541 INFO -------------------- Start --------------------\n",
      "2019-11-14 21:49:52,541 INFO Working directory: /mnt/data/git/cardio.\n",
      "2019-11-14 21:49:52,541 INFO Log file: ./logs/extract_segmentations_new_fast.log\n"
     ]
    }
   ],
   "source": [
    "lala_c = []\n",
    "lara_c = []\n",
    "\n",
    "for item in all_items:\n",
    "    if item.attrs.get('hash:key', False) == \"lalaContour\":\n",
    "        lala_c.append(item)\n",
    "    elif item.attrs.get('hash:key', False) == \"laraContour\":\n",
    "        lara_c.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-14 21:54:43,610 INFO extract all lalaContour segmentations\n",
      "2019-11-14 21:54:43,610 INFO building 2 contours\n",
      "2019-11-14 21:54:43,644 INFO extract all laraContour segmentations\n",
      "2019-11-14 21:54:43,645 INFO building 2 contours\n"
     ]
    }
   ],
   "source": [
    "class Contour():\n",
    "\n",
    "    def __init__(self, uid, image_size, pixel_size, points, sub_pixel_res, tag):\n",
    "        self.uid = uid\n",
    "        self.image_size = image_size\n",
    "        self.pixel_size = pixel_size\n",
    "        self.points = points\n",
    "        self.sub_pixel_res = sub_pixel_res\n",
    "        self.tag = tag\n",
    "\n",
    "    uid = 0\n",
    "    image_size = (0, 0)\n",
    "    pixel_size = (0, 0)\n",
    "    sub_pixel_res = 0\n",
    "    points = []\n",
    "\n",
    "def contour_factory(contours, tag):\n",
    "    \"\"\"\n",
    "    Expects a list of segmentation-elements\n",
    "    Reads/collects all neccessary data for each contour-element\n",
    "    Wraps each contour information in a contour-object,\n",
    "    builds a list of Contours and returns them\n",
    "    Params: list of contour-xml tags:\n",
    "    e.g. call:\n",
    "    xml_file = 'D:\\\\git\\\\cardio\\\\data\\\\processed\\\\Segmentations\\\\0AE4R74L190001011238291.3.12.2.1107.5.99.2.1013.30000008030612131778100072179.cvi42wsx'\n",
    "    with open(xml_file) as fp:\n",
    "    soup = BeautifulSoup(fp, 'lxml')\n",
    "    saendocardialContour = soup.find_all('hash:item', {'hash:key' : 'saendocardialContour'})\n",
    "\n",
    "    \"\"\"\n",
    "    cont = {}\n",
    "    logging.info('building {} contours'.format(len(contours)))\n",
    "    for contour in contours:\n",
    "        # print(contour)\n",
    "        uid = contour.parent.parent.attrs.get('hash:key')\n",
    "        image_width = contour.find('hash:item', {'hash:key': 'ImageSize'}).find('size:width').text\n",
    "        image_height = contour.find('hash:item', {'hash:key': 'ImageSize'}).find('size:height').text\n",
    "        pixel_width = contour.find('hash:item', {'hash:key': 'PixelSize'}).find('size:width').text\n",
    "        pixel_height = contour.find('hash:item', {'hash:key': 'PixelSize'}).find('size:height').text\n",
    "        sub_pixel = contour.find('hash:item', {'hash:key': 'SubpixelResolution'}).text\n",
    "        points_t = contour.find('hash:item', {'hash:key': 'Points'})\n",
    "\n",
    "        points_x = [int(int(point.text) / int(sub_pixel)) for point in points_t.find_all('point:x')]\n",
    "        points_y = [int(int(point.text) / int(sub_pixel)) for point in points_t.find_all('point:y')]\n",
    "\n",
    "        points = list(zip(points_x, points_y))\n",
    "\n",
    "        # create a Contour class for each contour\n",
    "        cont[str(uid)] = Contour(uid,\n",
    "                                 (image_width, image_height),\n",
    "                                 (pixel_width, pixel_height),\n",
    "                                 points,\n",
    "                                 sub_pixel, tag)\n",
    "    return cont\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Expects the Segmentation-XML-File as Beautifulsup-object\n",
    "    Returns: a dictionary with segmentation-tag : list of segmentation-tag elements\n",
    "    \"\"\"\n",
    "c = {}\n",
    "contours = {'lalaContour': [],\n",
    "            'laraContour': []}\n",
    "\n",
    "all_items = list(soup.find_all('hash:item'))\n",
    "lala_c = []\n",
    "lara_c = []\n",
    "\n",
    "for item in all_items:\n",
    "    if item.attrs.get('hash:key', False) == \"lalaContour\":\n",
    "        lala_c.append(item)\n",
    "    elif item.attrs.get('hash:key', False) == \"laraContour\":\n",
    "        lara_c.append(item)\n",
    "\n",
    "\n",
    "contours['lalaContour'] = lala_c\n",
    "contours['laraContour'] = lara_c\n",
    "\n",
    "\"\"\"\n",
    "convert to contour-objects\n",
    "Expects a dictionary object with:\n",
    "segmentation-tag : list of segmentation-tag elements\n",
    "converts the list of segmentation-elements\n",
    "into a flat dictionary with {uid : [contour]}\n",
    "\"\"\"\n",
    "\n",
    "for tag, contour_elements in contours.items():\n",
    "    if len(contour_elements) > 0:\n",
    "        logging.info('extract all {} segmentations'.format(tag))\n",
    "        # transform all contour elements in contour objects\n",
    "        contours = contour_factory(contour_elements, tag)\n",
    "        # update contours dictionary\n",
    "        # we might have multiple contours per slice\n",
    "        # we have a list of contours with tuples of (uid, contour-obj)\n",
    "        # we need to group all contours of one image with a list of contours\n",
    "        for uid, cont in contours.items():\n",
    "            # if this uid/image has already an\n",
    "            if c.get(uid, False):\n",
    "                c[uid].append(cont)\n",
    "            else:\n",
    "                c[uid] = [cont]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.3.46.670589.11.0.0.11.4.2.0.8365.5.2324.2007031307482373991': [<__main__.Contour at 0x7f3bc56dd748>,\n",
       "  <__main__.Contour at 0x7f3bc56dd978>],\n",
       " '1.3.46.670589.11.0.0.11.4.2.0.8365.5.2324.2007031307482321979': [<__main__.Contour at 0x7f3bc56dd7f0>,\n",
       "  <__main__.Contour at 0x7f3bc56dda58>]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {}\n",
    "contours = {'saendocardialContour': [],\n",
    "        'saepicardialContour': [],\n",
    "        'sarvendocardialContour': [],\n",
    "        'sarvepicardialContour': []}\n",
    "\n",
    "all_items = list(soup.find_all('hash:item'))\n",
    "myo_c = []\n",
    "rv_c = []\n",
    "lv_c = []\n",
    "for item in all_items:\n",
    "    if item.attrs.get('hash:key', False) == \"sarvendocardialContour\":\n",
    "        rv_c.append(item)\n",
    "    elif item.attrs.get('hash:key', False) == \"saendocardialContour\":\n",
    "        lv_c.append(item)\n",
    "    elif item.attrs.get('hash:key', False) == \"saepicardialContour\":\n",
    "        myo_c.append(item)\n",
    "\n",
    "contours['sarvendocardialContour'] = rv_c\n",
    "contours['saendocardialContour'] = lv_c\n",
    "contours['saepicardialContour'] = myo_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "import_circle",
   "language": "python",
   "name": "import_circle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 10:45:20,358 INFO -------------------- Start --------------------\n",
      "2020-12-18 10:45:20,359 INFO Working directory: /mnt/ssd/git/3d-mri-domain-adaption.\n",
      "2020-12-18 10:45:20,359 INFO Log file: ./logs/3D/ax_sax/temp/.log\n",
      "2020-12-18 10:45:20,359 INFO Log level for console: INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for root_dir and set working directory\n",
      "Working directory set to: /mnt/ssd/git/3d-mri-domain-adaption\n",
      "['/gpu:0', '/gpu:1']\n",
      "{'GPU_IDS': '0,1', 'GPUS': ['/gpu:0', '/gpu:1'], 'ARCHITECTURE': '3D', 'DATASET': 'GCN', 'FOLD': 0, 'EXP_NAME': 'ax_sax/temp/', 'EXPERIMENT': '3D/ax_sax/temp/', 'DATA_PATH_AX': '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/', 'DATA_PATH_AX2SAX': '/mnt/ssd/data/gcn/ax_sax_from_flo/SAX_3D/', 'DATA_PATH_SAX': '/mnt/ssd/data/gcn/ax_sax_from_flo/SAX_3D/', 'DATA_PATH_SAX2AX': '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/', 'DF_PATH': '/mnt/ssd/data/gcn/gcn_05_2020_ax_sax_86/folds.csv', 'MODEL_PATH': 'models/3D/ax_sax/temp/2020-12-18_10_45', 'TENSORBOARD_LOG_DIR': 'reports/tensorboard_logs/3D/ax_sax/temp/2020-12-18_10_45', 'CONFIG_PATH': 'reports/configs/3D/ax_sax/temp/2020-12-18_10_45', 'HISTORY_PATH': 'reports/history/3D/ax_sax/temp/2020-12-18_10_45', 'DIM': [80, 112, 112], 'DEPTH': 4, 'FILTERS': 16, 'SPACING': [3, 3, 3], 'M_POOL': [2, 2, 2], 'F_SIZE': [3, 3, 3], 'IMG_CHANNELS': 1, 'MASK_VALUES': [1, 2, 3], 'MASK_CLASSES': 3, 'BORDER_MODE': 4, 'IMG_INTERPOLATION': 1, 'MSK_INTERPOLATION': 0, 'AUGMENT': False, 'SHUFFLE': True, 'AUGMENT_GRID': False, 'RESAMPLE': True, 'SCALER': 'MinMax', 'AX_LOSS_WEIGHT': 10.0, 'WEIGHT_MSE_INPLANE': True, 'MASK_SMALLER_THAN_THRESHOLD': 0.001, 'SAX_LOSS_WEIGHT': 10.0, 'CYCLE_LOSS': True, 'FOCUS_LOSS_WEIGHT': 1.0, 'FOCUS_LOSS': True, 'USE_SAX2AX_PROB': False, 'MIN_UNET_PROBABILITY': 0.9, 'GENERATOR_WORKER': 2, 'SEED': 42, 'BATCHSIZE': 2, 'INITIAL_EPOCH': 0, 'EPOCHS': 300, 'EPOCHS_BETWEEN_CHECKPOINTS': 5, 'MONITOR_FUNCTION': 'val_loss', 'MONITOR_MODE': 'min', 'SAVE_MODEL_FUNCTION': 'val_loss', 'SAVE_MODEL_MODE': 'min', 'MODEL_PATIENCE': 20, 'BN_FIRST': False, 'BATCH_NORMALISATION': True, 'USE_UPSAMPLE': True, 'PAD': 'same', 'KERNEL_INIT': 'he_normal', 'OPTIMIZER': 'adam', 'ACTIVATION': 'elu', 'LEARNING_RATE': 0.0001, 'DECAY_FACTOR': 0.3, 'MIN_LR': 1e-10, 'LOSS_FUNCTION': <function bce_dice_loss at 0x7f2722d870d0>}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------define logging and working directory\n",
    "from ProjectRoot import change_wd_to_project_root\n",
    "change_wd_to_project_root()\n",
    "from src.utils.Notebook_imports import *\n",
    "from src.utils.Tensorflow_helper import choose_gpu_by_id\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import cv2\n",
    "# ------------------------------------------define GPU id/s to use\n",
    "GPU_IDS = '0,1'\n",
    "GPUS = choose_gpu_by_id(GPU_IDS)\n",
    "print(GPUS)\n",
    "# ------------------------------------------jupyter magic config\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# ------------------------------------------ import helpers\n",
    "from src.utils.Utils_io import Console_and_file_logger, init_config\n",
    "from src.visualization.Visualize import show_2D_or_3D\n",
    "from src.data.Dataset import get_img_msk_files_from_split_dir, load_acdc_files, get_train_data_from_df, get_trainings_files\n",
    "from src.data.Generators import DataGenerator, CycleMotionDataGenerator\n",
    "from src.utils.KerasCallbacks import get_callbacks\n",
    "import src.utils.Loss_and_metrics as metr\n",
    "import src.models.SpatialTransformer as st\n",
    "from src.models.SpatialTransformer import create_affine_cycle_transformer_model\n",
    "from src.models.ModelUtils import load_pretrained_model\n",
    "# ------------------------------------------path and project params\n",
    "ARCHITECTURE = '3D' # 2D\n",
    "DATASET = 'GCN'  # 'acdc' # or 'gcn' or different versions such as gcn_01/02\n",
    "FOLD = 0 # CV fold 0-3\n",
    "EXP_NAME = 'ax_sax/temp/' # Define an experiment name, could have subfolder conventions\n",
    "EXPERIMENT = '{}/{}'.format(ARCHITECTURE, EXP_NAME) # Uniform path names, separation of concerns\n",
    "timestemp = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")) # ad a timestep to each project to make repeated experiments unique\n",
    "\n",
    "# Our generator expects the following fix data structure (could be changed in src/data/Generators)\n",
    "# any-path/\n",
    "#    - AX_3D(anyname_img.nrrd and anyname_msk.nrrd)\n",
    "#    - AX_to_SAX_3D\n",
    "#    - SAX_3D\n",
    "#    - SAX_to_AX_3D\n",
    "DATA_PATH_AX = '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/' # path to AX 3D files\n",
    "DATA_PATH_AX2SAX = '/mnt/ssd/data/gcn/ax_sax_from_flo/SAX_3D/' # path to transformed AX 3D files (target of AX)\n",
    "\n",
    "DATA_PATH_SAX = '/mnt/ssd/data/gcn/ax_sax_from_flo/SAX_3D/' # path to SAX 3D files\n",
    "DATA_PATH_SAX2AX = '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/' # path to transformed SAX 3D files (target of SAX)\n",
    "\n",
    "DF_PATH = '/mnt/ssd/data/gcn/gcn_05_2020_ax_sax_86/folds.csv' # path to folds dataframe\n",
    "\n",
    "MODEL_PATH = os.path.join('models', EXPERIMENT, timestemp)\n",
    "TENSORBOARD_LOG_DIR = os.path.join('reports/tensorboard_logs', EXPERIMENT,timestemp)\n",
    "CONFIG_PATH = os.path.join('reports/configs/',EXPERIMENT,timestemp)\n",
    "HISTORY_PATH = os.path.join('reports/history/',EXPERIMENT,timestemp)\n",
    "\n",
    "# ------------------------------------------static model, loss and generator hyperparameters\n",
    "DIM = [80, 112, 112] # network input params for spacing of 3, (z,y,x)\n",
    "DEPTH = 4 # number of down-/upsampling blocks\n",
    "FILTERS = 16 # initial number of filters, will be doubled after each downsampling block\n",
    "SPACING = [3, 3, 3] # if resample, resample to this spacing, (z,y,x)\n",
    "M_POOL = [2, 2, 2]# size of max-pooling used for downsampling and upsampling\n",
    "F_SIZE = [3, 3, 3] # conv filter size\n",
    "IMG_CHANNELS = 1 # Currently our model needs that image channel\n",
    "MASK_VALUES = [1, 2, 3]  #channel order: Background, RV, MYO, LV\n",
    "MASK_CLASSES = len(MASK_VALUES) # no of labels\n",
    "BORDER_MODE = cv2.BORDER_REFLECT_101 # border mode for the data generation\n",
    "IMG_INTERPOLATION = cv2.INTER_LINEAR # image interpolation in the genarator\n",
    "MSK_INTERPOLATION = cv2.INTER_NEAREST # mask interpolation in the generator\n",
    "AUGMENT = False # Not implemented for the AX2SAX case\n",
    "SHUFFLE = True\n",
    "AUGMENT_GRID = False # Not implemented for the AX2SAX case\n",
    "RESAMPLE = True\n",
    "SCALER = 'MinMax' # MinMax Standard or Robust\n",
    "\n",
    "AX_LOSS_WEIGHT = 10.0 # weighting factor of the ax2sax loss\n",
    "WEIGHT_MSE_INPLANE = True # turn inplane weighting on/off\n",
    "MASK_SMALLER_THAN_THRESHOLD = 0.001 # define the threshold for masking the ax2sax/sax2ax MSE loss, areas with smaller values, will be masked out\n",
    "\n",
    "SAX_LOSS_WEIGHT = 10.0 # weighting factor of the sax2ax loss\n",
    "CYCLE_LOSS = True # turn this loss on/off\n",
    "\n",
    "FOCUS_LOSS_WEIGHT = 1.0 # weighting of the focus loss\n",
    "FOCUS_LOSS = True # turn this loss on/off\n",
    "USE_SAX2AX_PROB = False # apply the focus loss on AX2SAX_mask predictions, or on AX2SAX2AX_mask (back-transformed) predictions\n",
    "MIN_UNET_PROBABILITY = 0.9 # threshold to count only prediction greater than this value for the focus loss\n",
    "\n",
    "# ------------------------------------------individual training params\n",
    "GENERATOR_WORKER = 2 # number of parallel workers in our generator. if not set, use batchsize, numbers greater than batchsize does not make any sense\n",
    "SEED = 42 # define a seed for the generator shuffle\n",
    "BATCHSIZE = 2 # 32, 64, 24, 16, 1 for 3spacing 3,3,3 use: 2\n",
    "INITIAL_EPOCH = 0 # change this to continue training\n",
    "EPOCHS = 300 # define a maximum numbers of epochs\n",
    "EPOCHS_BETWEEN_CHECKPOINTS = 5\n",
    "MONITOR_FUNCTION = 'val_loss'\n",
    "MONITOR_MODE = 'min'\n",
    "SAVE_MODEL_FUNCTION = 'val_loss'\n",
    "SAVE_MODEL_MODE = 'min'\n",
    "MODEL_PATIENCE = 20\n",
    "BN_FIRST = False # decide if batch normalisation between conv and activation or afterwards\n",
    "BATCH_NORMALISATION = True # apply BN or not\n",
    "USE_UPSAMPLE = True # otherwise use transpose for upsampling\n",
    "PAD = 'same' # padding strategy of the conv layers\n",
    "KERNEL_INIT = 'he_normal' # conv weight initialisation\n",
    "OPTIMIZER = 'adam' # Adam, Adagrad, RMSprop, Adadelta,  # https://keras.io/optimizers/\n",
    "ACTIVATION = 'elu' # tf.keras.layers.LeakyReLU(), relu or any other non linear activation function\n",
    "LEARNING_RATE = 1e-4 # start with a huge lr to converge fast\n",
    "DECAY_FACTOR = 0.3 # Define a learning rate decay for the ReduceLROnPlateau callback\n",
    "MIN_LR = 1e-10 # minimal lr, smaller lr does not improve the model\n",
    "DROPOUT_min = 0.3 # lower dropout at the shallow layers\n",
    "DROPOUT_max = 0.5 # higher dropout at the deep layers\n",
    "\n",
    "# ------------------------------------------these metrics and loss function are meant if you continue training of the U-Net\n",
    "metrics = [\n",
    "    metr.dice_coef_labels,\n",
    "    metr.dice_coef_myo,\n",
    "    metr.dice_coef_lv,\n",
    "    metr.dice_coef_rv\n",
    "]\n",
    "LOSS_FUNCTION = metr.bce_dice_loss\n",
    "\n",
    "# Create a logger instance with the following setup: info or debug to console and file and error logs to a separate file\n",
    "# Define a config for param injection,\n",
    "# save a serialized version to load the experiment for prediction/evaluation, \n",
    "# make sure all paths exist\n",
    "Console_and_file_logger(EXPERIMENT, logging.INFO)\n",
    "config = init_config(config=locals(), save=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Tensorflow setup and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 10:45:21,098 INFO Is built with tensorflow: True\n",
      "2020-12-18 10:45:21,099 INFO Visible devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "2020-12-18 10:45:21,104 INFO Local devices: \n",
      " [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10918796336304155601\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17341752124371417084\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2124121724075268104\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15795377815234041702\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23089242496\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12597933188097921041\n",
      "physical_device_desc: \"device: 0, name: TITAN RTX, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23561743872\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11170506739787297098\n",
      "physical_device_desc: \"device: 1, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      "]\n",
      "2020-12-18 10:45:21,110 INFO Compute dtype: float16\n",
      "2020-12-18 10:45:21,110 INFO Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "logging.info('Is built with tensorflow: {}'.format(tf.test.is_built_with_cuda()))\n",
    "logging.info('Visible devices:\\n{}'.format(tf.config.list_physical_devices()))\n",
    "logging.info('Local devices: \\n {}'.format(device_lib.list_local_devices()))\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "logging.info('Compute dtype: %s' % policy.compute_dtype)\n",
    "logging.info('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trainings and validation files for the choosen fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 10:45:21,838 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/\n",
      "2020-12-18 10:45:21,838 INFO Patients train: 64\n",
      "2020-12-18 10:45:21,844 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-18 10:45:21,845 INFO AX train CMR: 120, AX train masks: 120\n",
      "2020-12-18 10:45:21,845 INFO AX val CMR: 42, AX val masks: 42\n",
      "2020-12-18 10:45:21,850 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/SAX_3D/\n",
      "2020-12-18 10:45:21,850 INFO Patients train: 64\n",
      "2020-12-18 10:45:21,856 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-18 10:45:21,857 INFO AX2SAX train CMR: 120, AX2SAX train masks: 120\n",
      "2020-12-18 10:45:21,857 INFO AX2SAX val CMR: 42, AX2SAX val masks: 42\n",
      "2020-12-18 10:45:21,862 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/SAX_3D/\n",
      "2020-12-18 10:45:21,863 INFO Patients train: 64\n",
      "2020-12-18 10:45:21,872 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-18 10:45:21,872 INFO SAX train CMR: 120, SAX train masks: 120\n",
      "2020-12-18 10:45:21,873 INFO SAX val CMR: 42, SAX val masks: 42\n",
      "2020-12-18 10:45:21,877 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/\n",
      "2020-12-18 10:45:21,878 INFO Patients train: 64\n",
      "2020-12-18 10:45:21,887 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-18 10:45:21,887 INFO SAX2AX train CMR: 120, SAX2AX train masks: 120\n",
      "2020-12-18 10:45:21,888 INFO SAX2AX val CMR: 42, SAX2AX val masks: 42\n"
     ]
    }
   ],
   "source": [
    "# Load AX volumes\n",
    "x_train_ax, y_train_ax, x_val_ax, y_val_ax =  get_trainings_files(data_path=DATA_PATH_AX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('AX train CMR: {}, AX train masks: {}'.format(len(x_train_ax), len(y_train_ax)))\n",
    "logging.info('AX val CMR: {}, AX val masks: {}'.format(len(x_val_ax), len(y_val_ax)))\n",
    "\n",
    "# load AX2SAX volumes\n",
    "x_train_ax2sax, y_train_ax2sax, x_val_ax2sax, y_val_ax2sax =  get_trainings_files(data_path=DATA_PATH_AX2SAX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('AX2SAX train CMR: {}, AX2SAX train masks: {}'.format(len(x_train_ax2sax), len(y_train_ax2sax)))\n",
    "logging.info('AX2SAX val CMR: {}, AX2SAX val masks: {}'.format(len(x_val_ax2sax), len(y_val_ax2sax)))\n",
    "\n",
    "# Load SAX volumes\n",
    "x_train_sax, y_train_sax, x_val_sax, y_val_sax =  get_trainings_files(data_path=DATA_PATH_SAX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('SAX train CMR: {}, SAX train masks: {}'.format(len(x_train_sax), len(y_train_sax)))\n",
    "logging.info('SAX val CMR: {}, SAX val masks: {}'.format(len(x_val_sax), len(y_val_sax)))\n",
    "\n",
    "# load SAX2AX volumes\n",
    "x_train_sax2ax, y_train_sax2ax, x_val_sax2ax, y_val_sax2ax =  get_trainings_files(data_path=DATA_PATH_SAX2AX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('SAX2AX train CMR: {}, SAX2AX train masks: {}'.format(len(x_train_sax2ax), len(y_train_sax2ax)))\n",
    "logging.info('SAX2AX val CMR: {}, SAX2AX val masks: {}'.format(len(x_val_sax2ax), len(y_val_sax2ax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter files by name, debugging purpose\n",
    "#x_val_ax = [x for x in x_val_ax if '4A4PVCYL_2006' in x]\n",
    "#x_val_sax = [x for x in x_val_sax if '4A4PVCYL_2006' in x]\n",
    "#y_val_ax = [x for x in y_val_ax if '4A4PVCYL_2006' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-18 10:45:22,856 INFO Create DataGenerator\n",
      "2020-12-18 10:45:22,858 INFO generator in debug mode = False\n",
      "2020-12-18 10:45:22,858 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 2\n",
      " Scaler: MinMax\n",
      " Images: 120 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-18 10:45:22,858 INFO No augmentation\n",
      "2020-12-18 10:45:22,859 INFO Create DataGenerator\n",
      "2020-12-18 10:45:22,860 INFO generator in debug mode = False\n",
      "2020-12-18 10:45:22,860 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 2\n",
      " Scaler: MinMax\n",
      " Images: 42 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-18 10:45:22,860 INFO No augmentation\n"
     ]
    }
   ],
   "source": [
    "# create two generators, one for the training files, one for the validation files\n",
    "batch_generator = CycleMotionDataGenerator(x=x_train_ax, y=x_train_ax2sax, x2=x_train_sax, y2=x_train_sax2ax, config=config)\n",
    "valid_config = config.copy()\n",
    "valid_config['AUGMENT_GRID'] = False\n",
    "valid_config['AUGMENT'] = False\n",
    "valid_generator = CycleMotionDataGenerator(x=x_val_ax, y=x_val_ax2sax, x2=x_val_sax, y2=x_val_sax2ax, config=valid_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347b99e6163c4cb0b8afe7898f5ace68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='batch', max=21), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select batch generator output\n",
    "x = ''\n",
    "y = ''\n",
    "@interact\n",
    "def select_batch(batch = (0,len(valid_generator), 1)):\n",
    "    global x, y, x2, y2\n",
    "    input_ , output_ = valid_generator.__getitem__(batch)\n",
    "    x = input_[0]\n",
    "    y = output_[0]\n",
    "    x2 = input_[1]\n",
    "    y2 = output_[1]\n",
    "    logging.info('input elements: {}'.format(len(input_)))\n",
    "    logging.info('output elements: {}'.format(len(output_)))\n",
    "    logging.info(x.shape)\n",
    "    logging.info(y.shape)\n",
    "    logging.info(x2.shape)\n",
    "    logging.info(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0737b931e094b0aa9e84dc79e7bd1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='im', max=1), IntSlider(value=3, description='slice_by', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def select_image_in_batch(im = (0,x.shape[0]- 1, 1),slice_by=(1,6)):\n",
    "    \n",
    "    # define a different logging level to make the generator steps visible\n",
    "    #logging.getLogger().setLevel(logging.DEBUG)\n",
    "    temp_dir = 'reports/figures/temp/'\n",
    "    ensure_dir(temp_dir)\n",
    "\n",
    "    logging.info('AX: {}'.format(x[im].shape))\n",
    "    show_2D_or_3D(x[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'ax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('AXtoSAX: {}'.format(y[im].shape))\n",
    "    show_2D_or_3D(y[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'ax2sax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('SAX: {}'.format(x2[im].shape))\n",
    "    show_2D_or_3D(x2[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'sax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('SAXtoAX: {}'.format(y2[im].shape))\n",
    "    show_2D_or_3D(y2[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'sax2ax.pdf'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 18:21:07,438 INFO Load model from Experiment: 2D/gcn_05_2020_sax_excl_ax_patients\n",
      "2020-12-17 18:21:07,440 INFO load model with keras api\n",
      "2020-12-17 18:21:09,791 INFO Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "2020-12-17 18:21:09,792 INFO Keras API failed, use json repr. load model from: models/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/model.json .\n",
      "2020-12-17 18:21:09,793 INFO loading model description\n",
      "2020-12-17 18:21:10,720 INFO loading model weights\n",
      "2020-12-17 18:21:10,884 INFO model models/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/model.json loaded\n"
     ]
    }
   ],
   "source": [
    "# load a pretrained 2D unet\n",
    "\"\"\"\n",
    "load past config for model training \n",
    "\"\"\"\n",
    "if 'config_chooser' in locals():\n",
    "    config_file  = config_chooser.selected\n",
    "else:\n",
    "    #config_file = '/mnt/ssd/git/3d-mri-domain-adaption/reports/configs/2D/gcn_and_acdc_excl_ax/config.json' # config for TMI paper\n",
    "    config_file = '/mnt/ssd/git/cardio/reports/configs/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/config.json' # retrained with downsampling\n",
    "\n",
    "# load config with all params into global namespace\n",
    "with open(config_file, encoding='utf-8') as data_file:\n",
    "    config_temp = json.loads(data_file.read())\n",
    "logging.info('Load model from Experiment: {}'.format(config_temp['EXPERIMENT']))\n",
    "\n",
    "if 'strategy' not in globals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "with strategy.scope():\n",
    "    globals()['unet'] = load_pretrained_model(config_temp, metrics, comp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 18:21:13,224 INFO unet given, use it to max probability\n",
      "2020-12-17 18:21:32,288 INFO adding ax2sax MSE loss with a weighting of 10.0\n",
      "2020-12-17 18:21:32,288 INFO adding cycle loss with a weighting of 10.0\n",
      "2020-12-17 18:21:32,289 INFO adding focus loss on mask_prob with a weighting of 1.0\n"
     ]
    }
   ],
   "source": [
    "if 'strategy' not in globals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "# inject the pre-trained unet if given, otherwise build the model without the pretrained unet\n",
    "with strategy.scope():\n",
    "    model = st.create_affine_cycle_transformer_model(config=config, unet=locals().get('unet', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"affine_cycle_transformer\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             [(None, 80, 112, 112, 1)]        0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv_encoder (ConvEncoder)                       ((None, 5, 7, 7, 256), [(None, 8 3537424           input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "global_average_pooling3d (GlobalAveragePooling3D (None, 256)                      0                 conv_encoder[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense1 (Dense)                                   (None, 256)                      65792             global_average_pooling3d[0][0]                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense2 (Dense)                                   (None, 9)                        2313              dense1[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (TensorFlowOpLayer)  [(None, 3)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (TensorFlowOpLayer)  [(None, 3)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                        (None, 6)                        0                 tf_op_layer_strided_slice_1[0][0]                 \n",
      "                                                                                                    tf_op_layer_strided_slice_2[0][0]                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (TensorFlowOpLayer)    [(None, 6)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_mod_matrix (Euler2Matrix)                 (None, 12)                       0                 concatenate[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_matrix (Euler2Matrix)                     (None, 12)                       0                 tf_op_layer_strided_slice[0][0]                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_mod_st (SpatialTransformer)               (None, 80, 112, 112, 1)          0                 input_1[0][0]                                     \n",
      "                                                                                                    ax2sax_mod_matrix[0][0]                           \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                             [(None, 80, 112, 112, 1)]        0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "inverse3d_matrix (Inverse3DMatrix)               (None, 12)                       0                 ax2sax_matrix[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "mask_prob (UnetWrapper)                          (None, 80, 112, 112, 3)          19432275          ax2sax_mod_st[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "inverse3d_matrix_1 (Inverse3DMatrix)             (None, 12)                       0                 ax2sax_mod_matrix[0][0]                           \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax (SpatialTransformer)                      (None, 80, 112, 112, 1)          0                 input_1[0][0]                                     \n",
      "                                                                                                    ax2sax_matrix[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "sax2ax (SpatialTransformer)                      (None, 80, 112, 112, 1)          0                 input_2[0][0]                                     \n",
      "                                                                                                    inverse3d_matrix[0][0]                            \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "mask2ax (SpatialTransformer)                     (None, 80, 112, 112, 3)          0                 mask_prob[0][0]                                   \n",
      "                                                                                                    inverse3d_matrix_1[0][0]                          \n",
      "======================================================================================================================================================\n",
      "Total params: 23,037,804\n",
      "Trainable params: 3,603,545\n",
      "Non-trainable params: 19,434,259\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=150)\n",
    "#plot_model(model, to_file='reports/figures/temp_graph.pdf',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ef138eec37498fa7220d7be9fe2eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='im', max=1), Text(value='0.001', description='mask_small…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def select_image_in_batch(im = (0,x.shape[0]- 1, 1),mask_smaller_than='0.001', slice_by=(1,6)):\n",
    "    global m\n",
    "    import numpy as np\n",
    "    temp = x[im]\n",
    "    sax = x2[im]\n",
    "    temp_ = y[im]\n",
    "    \n",
    "    mask = temp_ >float(mask_smaller_than)\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on: {}'.format(temp.shape))\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    pred, inv_pred, ax2sax_mod, prob, ax_msk,m, m_mod = model.predict(x = [np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])                     \n",
    "    logging.info('rotated by the model: {}'.format(pred[0].shape))\n",
    "    show_2D_or_3D(pred[0][::slice_by], mask[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverse rotation on SAX: {}'.format(inv_pred[0].shape))\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask: {}'.format(inv_pred[0].shape))\n",
    "    show_2D_or_3D(prob[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask in ax: {}'.format(ax_msk[0].shape))\n",
    "    show_2D_or_3D(ax_msk[0][::slice_by])\n",
    "    plt.show()\n",
    "    \n",
    "    # calculate the loss mask from target AX2SAX image\n",
    "    mask = temp_ >float(mask_smaller_than)\n",
    "    logging.info('masked by GT: {}'.format(mask.shape))\n",
    "    masked = pred[0] * mask\n",
    "    show_2D_or_3D(masked[::slice_by], mask[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (AX2SAX): {}'.format(temp_.shape))\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('Created MSE mask by thresholding the target (AX2SAX) with {}: {}'.format(mask_smaller_than,temp_.shape))\n",
    "    show_2D_or_3D(mask[::slice_by])\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        from tensorflow.keras.metrics import MSE as mse\n",
    "        logging.info('MSE: {}'.format(mse(pred[0], temp_).numpy().mean()))\n",
    "        logging.info('prob loss: {}'.format(metr.max_volume_loss(min_probabillity=0.5)(temp_[tf.newaxis,...],prob).numpy().mean()))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "        print(np.reshape(m_mod[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-17 18:22:26,158 INFO Fit model, start trainings process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 24.4122 - ax2sax_loss: 1.0678 - sax2ax_loss: 1.2741 - mask_prob_loss: 0.9925\n",
      "Epoch 00001: val_loss improved from inf to 23.18034, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 168s 3s/step - loss: 24.4122 - ax2sax_loss: 1.0678 - sax2ax_loss: 1.2741 - mask_prob_loss: 0.9925 - val_loss: 23.1803 - val_ax2sax_loss: 1.0128 - val_sax2ax_loss: 1.2061 - val_mask_prob_loss: 0.9913 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 23.9050 - ax2sax_loss: 1.0417 - sax2ax_loss: 1.2496 - mask_prob_loss: 0.9925\n",
      "Epoch 00002: val_loss improved from 23.18034 to 22.82158, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 121s 2s/step - loss: 23.9050 - ax2sax_loss: 1.0417 - sax2ax_loss: 1.2496 - mask_prob_loss: 0.9925 - val_loss: 22.8216 - val_ax2sax_loss: 0.9924 - val_sax2ax_loss: 1.1907 - val_mask_prob_loss: 0.9906 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 23.7573 - ax2sax_loss: 1.0340 - sax2ax_loss: 1.2425 - mask_prob_loss: 0.9921\n",
      "Epoch 00003: val_loss improved from 22.82158 to 22.17648, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 126s 2s/step - loss: 23.7573 - ax2sax_loss: 1.0340 - sax2ax_loss: 1.2425 - mask_prob_loss: 0.9921 - val_loss: 22.1765 - val_ax2sax_loss: 0.9575 - val_sax2ax_loss: 1.1612 - val_mask_prob_loss: 0.9898 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 22.9219 - ax2sax_loss: 0.9924 - sax2ax_loss: 1.2007 - mask_prob_loss: 0.9912\n",
      "Epoch 00004: val_loss improved from 22.17648 to 21.27011, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 126s 2s/step - loss: 22.9219 - ax2sax_loss: 0.9924 - sax2ax_loss: 1.2007 - mask_prob_loss: 0.9912 - val_loss: 21.2701 - val_ax2sax_loss: 0.9200 - val_sax2ax_loss: 1.1082 - val_mask_prob_loss: 0.9888 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 21.0728 - ax2sax_loss: 0.9090 - sax2ax_loss: 1.0993 - mask_prob_loss: 0.9899\n",
      "Epoch 00005: val_loss improved from 21.27011 to 21.19098, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 125s 2s/step - loss: 21.0728 - ax2sax_loss: 0.9090 - sax2ax_loss: 1.0993 - mask_prob_loss: 0.9899 - val_loss: 21.1910 - val_ax2sax_loss: 0.9165 - val_sax2ax_loss: 1.1036 - val_mask_prob_loss: 0.9901 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 20.5951 - ax2sax_loss: 0.8841 - sax2ax_loss: 1.0764 - mask_prob_loss: 0.9900\n",
      "Epoch 00006: val_loss improved from 21.19098 to 20.61299, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 128s 2s/step - loss: 20.5951 - ax2sax_loss: 0.8841 - sax2ax_loss: 1.0764 - mask_prob_loss: 0.9900 - val_loss: 20.6130 - val_ax2sax_loss: 0.8955 - val_sax2ax_loss: 1.0668 - val_mask_prob_loss: 0.9899 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 20.1931 - ax2sax_loss: 0.8661 - sax2ax_loss: 1.0542 - mask_prob_loss: 0.9901\n",
      "Epoch 00007: val_loss improved from 20.61299 to 20.30437, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 126s 2s/step - loss: 20.1931 - ax2sax_loss: 0.8661 - sax2ax_loss: 1.0542 - mask_prob_loss: 0.9901 - val_loss: 20.3044 - val_ax2sax_loss: 0.8841 - val_sax2ax_loss: 1.0474 - val_mask_prob_loss: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 19.8368 - ax2sax_loss: 0.8491 - sax2ax_loss: 1.0356 - mask_prob_loss: 0.9903\n",
      "Epoch 00008: val_loss improved from 20.30437 to 19.96790, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 127s 2s/step - loss: 19.8368 - ax2sax_loss: 0.8491 - sax2ax_loss: 1.0356 - mask_prob_loss: 0.9903 - val_loss: 19.9679 - val_ax2sax_loss: 0.8672 - val_sax2ax_loss: 1.0306 - val_mask_prob_loss: 0.9898 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 19.5599 - ax2sax_loss: 0.8362 - sax2ax_loss: 1.0207 - mask_prob_loss: 0.9905\n",
      "Epoch 00009: val_loss improved from 19.96790 to 19.57837, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 125s 2s/step - loss: 19.5599 - ax2sax_loss: 0.8362 - sax2ax_loss: 1.0207 - mask_prob_loss: 0.9905 - val_loss: 19.5784 - val_ax2sax_loss: 0.8459 - val_sax2ax_loss: 1.0130 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 19.3197 - ax2sax_loss: 0.8229 - sax2ax_loss: 1.0100 - mask_prob_loss: 0.9905\n",
      "Epoch 00010: val_loss improved from 19.57837 to 19.54302, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 127s 2s/step - loss: 19.3197 - ax2sax_loss: 0.8229 - sax2ax_loss: 1.0100 - mask_prob_loss: 0.9905 - val_loss: 19.5430 - val_ax2sax_loss: 0.8452 - val_sax2ax_loss: 1.0101 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 19.1171 - ax2sax_loss: 0.8128 - sax2ax_loss: 0.9999 - mask_prob_loss: 0.9903\n",
      "Epoch 00011: val_loss improved from 19.54302 to 19.42570, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 128s 2s/step - loss: 19.1171 - ax2sax_loss: 0.8128 - sax2ax_loss: 0.9999 - mask_prob_loss: 0.9903 - val_loss: 19.4257 - val_ax2sax_loss: 0.8386 - val_sax2ax_loss: 1.0050 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 19.0067 - ax2sax_loss: 0.8074 - sax2ax_loss: 0.9942 - mask_prob_loss: 0.9903\n",
      "Epoch 00012: val_loss improved from 19.42570 to 19.41206, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 126s 2s/step - loss: 19.0067 - ax2sax_loss: 0.8074 - sax2ax_loss: 0.9942 - mask_prob_loss: 0.9903 - val_loss: 19.4121 - val_ax2sax_loss: 0.8376 - val_sax2ax_loss: 1.0047 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9893 - ax2sax_loss: 0.8056 - sax2ax_loss: 0.9943 - mask_prob_loss: 0.9903\n",
      "Epoch 00013: val_loss improved from 19.41206 to 19.34957, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9893 - ax2sax_loss: 0.8056 - sax2ax_loss: 0.9943 - mask_prob_loss: 0.9903 - val_loss: 19.3496 - val_ax2sax_loss: 0.8333 - val_sax2ax_loss: 1.0027 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9576 - ax2sax_loss: 0.8044 - sax2ax_loss: 0.9924 - mask_prob_loss: 0.9903\n",
      "Epoch 00014: val_loss did not improve from 19.34957\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9576 - ax2sax_loss: 0.8044 - sax2ax_loss: 0.9924 - mask_prob_loss: 0.9903 - val_loss: 19.5379 - val_ax2sax_loss: 0.8423 - val_sax2ax_loss: 1.0125 - val_mask_prob_loss: 0.9898 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9520 - ax2sax_loss: 0.8041 - sax2ax_loss: 0.9921 - mask_prob_loss: 0.9903\n",
      "Epoch 00015: val_loss did not improve from 19.34957\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9520 - ax2sax_loss: 0.8041 - sax2ax_loss: 0.9921 - mask_prob_loss: 0.9903 - val_loss: 19.4294 - val_ax2sax_loss: 0.8367 - val_sax2ax_loss: 1.0073 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9556 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9911 - mask_prob_loss: 0.9903\n",
      "Epoch 00016: val_loss did not improve from 19.34957\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9556 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9911 - mask_prob_loss: 0.9903 - val_loss: 19.4622 - val_ax2sax_loss: 0.8390 - val_sax2ax_loss: 1.0082 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9770 - ax2sax_loss: 0.8061 - sax2ax_loss: 0.9926 - mask_prob_loss: 0.9903\n",
      "Epoch 00017: val_loss did not improve from 19.34957\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9770 - ax2sax_loss: 0.8061 - sax2ax_loss: 0.9926 - mask_prob_loss: 0.9903 - val_loss: 19.3710 - val_ax2sax_loss: 0.8352 - val_sax2ax_loss: 1.0030 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9614 - ax2sax_loss: 0.8053 - sax2ax_loss: 0.9918 - mask_prob_loss: 0.9903\n",
      "Epoch 00018: val_loss improved from 19.34957 to 19.25380, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9614 - ax2sax_loss: 0.8053 - sax2ax_loss: 0.9918 - mask_prob_loss: 0.9903 - val_loss: 19.2538 - val_ax2sax_loss: 0.8300 - val_sax2ax_loss: 0.9964 - val_mask_prob_loss: 0.9895 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9456 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9901 - mask_prob_loss: 0.9904\n",
      "Epoch 00019: val_loss did not improve from 19.25380\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9456 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9901 - mask_prob_loss: 0.9904 - val_loss: 19.3496 - val_ax2sax_loss: 0.8343 - val_sax2ax_loss: 1.0017 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9498 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9906 - mask_prob_loss: 0.9903\n",
      "Epoch 00020: val_loss did not improve from 19.25380\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9498 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9906 - mask_prob_loss: 0.9903 - val_loss: 19.3739 - val_ax2sax_loss: 0.8346 - val_sax2ax_loss: 1.0039 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9532 - ax2sax_loss: 0.8056 - sax2ax_loss: 0.9907 - mask_prob_loss: 0.9904\n",
      "Epoch 00021: val_loss did not improve from 19.25380\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9532 - ax2sax_loss: 0.8056 - sax2ax_loss: 0.9907 - mask_prob_loss: 0.9904 - val_loss: 19.3257 - val_ax2sax_loss: 0.8332 - val_sax2ax_loss: 1.0004 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9338 - ax2sax_loss: 0.8050 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9903\n",
      "Epoch 00022: val_loss did not improve from 19.25380\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9338 - ax2sax_loss: 0.8050 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9903 - val_loss: 19.3568 - val_ax2sax_loss: 0.8357 - val_sax2ax_loss: 1.0010 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9529 - ax2sax_loss: 0.8051 - sax2ax_loss: 0.9911 - mask_prob_loss: 0.9904\n",
      "Epoch 00023: val_loss improved from 19.25380 to 19.19680, saving model to models/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_18_21/model.h5\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9529 - ax2sax_loss: 0.8051 - sax2ax_loss: 0.9911 - mask_prob_loss: 0.9904 - val_loss: 19.1968 - val_ax2sax_loss: 0.8279 - val_sax2ax_loss: 0.9929 - val_mask_prob_loss: 0.9894 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9437 - ax2sax_loss: 0.8053 - sax2ax_loss: 0.9900 - mask_prob_loss: 0.9904\n",
      "Epoch 00024: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9437 - ax2sax_loss: 0.8053 - sax2ax_loss: 0.9900 - mask_prob_loss: 0.9904 - val_loss: 19.2871 - val_ax2sax_loss: 0.8322 - val_sax2ax_loss: 0.9976 - val_mask_prob_loss: 0.9895 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9591 - ax2sax_loss: 0.8063 - sax2ax_loss: 0.9906 - mask_prob_loss: 0.9904\n",
      "Epoch 00025: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9591 - ax2sax_loss: 0.8063 - sax2ax_loss: 0.9906 - mask_prob_loss: 0.9904 - val_loss: 19.2367 - val_ax2sax_loss: 0.8297 - val_sax2ax_loss: 0.9950 - val_mask_prob_loss: 0.9895 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9442 - ax2sax_loss: 0.8053 - sax2ax_loss: 0.9900 - mask_prob_loss: 0.9904\n",
      "Epoch 00026: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9442 - ax2sax_loss: 0.8053 - sax2ax_loss: 0.9900 - mask_prob_loss: 0.9904 - val_loss: 19.4439 - val_ax2sax_loss: 0.8387 - val_sax2ax_loss: 1.0068 - val_mask_prob_loss: 0.9897 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9732 - ax2sax_loss: 0.8065 - sax2ax_loss: 0.9918 - mask_prob_loss: 0.9903\n",
      "Epoch 00027: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9732 - ax2sax_loss: 0.8065 - sax2ax_loss: 0.9918 - mask_prob_loss: 0.9903 - val_loss: 19.3343 - val_ax2sax_loss: 0.8332 - val_sax2ax_loss: 1.0013 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9471 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9902 - mask_prob_loss: 0.9904\n",
      "Epoch 00028: val_loss did not improve from 19.19680\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9471 - ax2sax_loss: 0.8054 - sax2ax_loss: 0.9902 - mask_prob_loss: 0.9904 - val_loss: 19.3675 - val_ax2sax_loss: 0.8352 - val_sax2ax_loss: 1.0026 - val_mask_prob_loss: 0.9896 - lr: 3.0000e-05\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9353 - ax2sax_loss: 0.8050 - sax2ax_loss: 0.9895 - mask_prob_loss: 0.9903\n",
      "Epoch 00029: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 129s 2s/step - loss: 18.9353 - ax2sax_loss: 0.8050 - sax2ax_loss: 0.9895 - mask_prob_loss: 0.9903 - val_loss: 19.2759 - val_ax2sax_loss: 0.8314 - val_sax2ax_loss: 0.9973 - val_mask_prob_loss: 0.9896 - lr: 3.0000e-05\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9345 - ax2sax_loss: 0.8050 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9904\n",
      "Epoch 00030: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9345 - ax2sax_loss: 0.8050 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9904 - val_loss: 19.2618 - val_ax2sax_loss: 0.8307 - val_sax2ax_loss: 0.9965 - val_mask_prob_loss: 0.9895 - lr: 3.0000e-05\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9314 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9903\n",
      "Epoch 00031: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9314 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9903 - val_loss: 19.2592 - val_ax2sax_loss: 0.8306 - val_sax2ax_loss: 0.9963 - val_mask_prob_loss: 0.9895 - lr: 3.0000e-05\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9299 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9890 - mask_prob_loss: 0.9903\n",
      "Epoch 00032: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9299 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9890 - mask_prob_loss: 0.9903 - val_loss: 19.2660 - val_ax2sax_loss: 0.8310 - val_sax2ax_loss: 0.9967 - val_mask_prob_loss: 0.9895 - lr: 3.0000e-05\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9304 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903\n",
      "Epoch 00033: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9304 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903 - val_loss: 19.2834 - val_ax2sax_loss: 0.8316 - val_sax2ax_loss: 0.9978 - val_mask_prob_loss: 0.9895 - lr: 3.0000e-05\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9348 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9895 - mask_prob_loss: 0.9903\n",
      "Epoch 00034: val_loss did not improve from 19.19680\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "61/61 [==============================] - 127s 2s/step - loss: 18.9348 - ax2sax_loss: 0.8049 - sax2ax_loss: 0.9895 - mask_prob_loss: 0.9903 - val_loss: 19.2433 - val_ax2sax_loss: 0.8298 - val_sax2ax_loss: 0.9956 - val_mask_prob_loss: 0.9895 - lr: 9.0000e-06\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9289 - ax2sax_loss: 0.8048 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903\n",
      "Epoch 00035: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9289 - ax2sax_loss: 0.8048 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903 - val_loss: 19.2650 - val_ax2sax_loss: 0.8306 - val_sax2ax_loss: 0.9969 - val_mask_prob_loss: 0.9895 - lr: 9.0000e-06\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9300 - ax2sax_loss: 0.8048 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9903\n",
      "Epoch 00036: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 128s 2s/step - loss: 18.9300 - ax2sax_loss: 0.8048 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9903 - val_loss: 19.2778 - val_ax2sax_loss: 0.8312 - val_sax2ax_loss: 0.9976 - val_mask_prob_loss: 0.9895 - lr: 9.0000e-06\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9291 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9904\n",
      "Epoch 00037: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9291 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9904 - val_loss: 19.2767 - val_ax2sax_loss: 0.8311 - val_sax2ax_loss: 0.9976 - val_mask_prob_loss: 0.9895 - lr: 9.0000e-06\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9283 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903\n",
      "Epoch 00038: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 125s 2s/step - loss: 18.9283 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903 - val_loss: 19.2926 - val_ax2sax_loss: 0.8318 - val_sax2ax_loss: 0.9985 - val_mask_prob_loss: 0.9896 - lr: 9.0000e-06\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9279 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903\n",
      "Epoch 00039: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9279 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9891 - mask_prob_loss: 0.9903 - val_loss: 19.2969 - val_ax2sax_loss: 0.8320 - val_sax2ax_loss: 0.9987 - val_mask_prob_loss: 0.9895 - lr: 9.0000e-06\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9318 - ax2sax_loss: 0.8048 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9904\n",
      "Epoch 00040: val_loss did not improve from 19.19680\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
      "61/61 [==============================] - 124s 2s/step - loss: 18.9318 - ax2sax_loss: 0.8048 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9904 - val_loss: 19.3003 - val_ax2sax_loss: 0.8322 - val_sax2ax_loss: 0.9989 - val_mask_prob_loss: 0.9896 - lr: 2.7000e-06\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9302 - ax2sax_loss: 0.8046 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9903\n",
      "Epoch 00041: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9302 - ax2sax_loss: 0.8046 - sax2ax_loss: 0.9894 - mask_prob_loss: 0.9903 - val_loss: 19.3071 - val_ax2sax_loss: 0.8325 - val_sax2ax_loss: 0.9993 - val_mask_prob_loss: 0.9896 - lr: 2.7000e-06\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9284 - ax2sax_loss: 0.8046 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9903\n",
      "Epoch 00042: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 126s 2s/step - loss: 18.9284 - ax2sax_loss: 0.8046 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9903 - val_loss: 19.2989 - val_ax2sax_loss: 0.8321 - val_sax2ax_loss: 0.9988 - val_mask_prob_loss: 0.9896 - lr: 2.7000e-06\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9298 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9904\n",
      "Epoch 00043: val_loss did not improve from 19.19680\n",
      "61/61 [==============================] - 125s 2s/step - loss: 18.9298 - ax2sax_loss: 0.8047 - sax2ax_loss: 0.9892 - mask_prob_loss: 0.9904 - val_loss: 19.3100 - val_ax2sax_loss: 0.8326 - val_sax2ax_loss: 0.9995 - val_mask_prob_loss: 0.9896 - lr: 2.7000e-06\n",
      "Epoch 00043: early stopping\n"
     ]
    }
   ],
   "source": [
    "# train one model\n",
    "initial_epoch = 0\n",
    "logging.info('Fit model, start trainings process')\n",
    "# fit model with trainingsgenerator\n",
    "results = model.fit(\n",
    "    x=batch_generator,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=len(valid_generator),\n",
    "    epochs=200,\n",
    "    callbacks = get_callbacks(config, valid_generator),\n",
    "    steps_per_epoch = len(batch_generator),\n",
    "    initial_epoch=initial_epoch,\n",
    "    max_queue_size=20,\n",
    "    workers=2,\n",
    "    use_multiprocessing=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if, for any reason, you want to save the latest model, use this cell\n",
    "#tf.keras.models.save_model(model,filepath=config['MODEL_PATH'],overwrite=True,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['MODEL_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 14:30:29,323 INFO Load model from Experiment: 3D/ax_sax/unetwithdownsamplingaugmentation_new_data\n",
      "2020-12-07 14:30:30,068 INFO unet given, use it to max probability\n",
      "2020-12-07 14:30:48,204 INFO adding ax2sax MSE loss with a weighting of 10.0\n",
      "2020-12-07 14:30:48,205 INFO adding cycle loss with a weighting of 10.0\n",
      "2020-12-07 14:30:48,206 INFO adding focus loss on mask_prob with a weighting of 1.0\n",
      "2020-12-07 14:30:48,484 INFO loaded model weights as h5 file\n"
     ]
    }
   ],
   "source": [
    "# Fast tests of a trained model, the \"real\" predictions will be done in src/notebooks/Predict\n",
    "\n",
    "\"\"\"\n",
    "load past config for model training \n",
    "\"\"\"\n",
    "if 'strategy' not in locals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "\n",
    "# round the crop and pad values instead of ceil\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-03_18_20/config.json' # Fold 0\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-03_22_02/config.json' # Fold 1\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-04_16_56/config.json' # Fold 2\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/config.json' # Fold 3\n",
    "\n",
    "config_file = 'reports/configs/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_11_44/config.json' # Fold 0\n",
    "\n",
    "\n",
    "\n",
    "# load a pre-trained ax2sax model, create the graph and load the weights separately, due to own loss functions, this is easier\n",
    "with open(config_file, encoding='utf-8') as data_file:\n",
    "    config_temp = json.loads(data_file.read())\n",
    "config_temp['LOSS_FUNCTION'] = config['LOSS_FUNCTION']\n",
    "logging.info('Load model from Experiment: {}'.format(config_temp['EXPERIMENT']))\n",
    "\n",
    "with strategy.scope():\n",
    "    globals()['model'] = st.create_affine_cycle_transformer_model(config=config_temp, metrics=metrics, unet=locals().get('unet', None))\n",
    "    model.load_weights(os.path.join(config_temp['MODEL_PATH'],'model.h5'))\n",
    "    logging.info('loaded model weights as h5 file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast predictions with all files of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 20:07:27,707 INFO Create DataGenerator\n",
      "2020-12-03 20:07:27,708 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 10\n",
      " Scaler: MinMax\n",
      " Images: 120 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-03 20:07:27,709 INFO No augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcda926c12e41d386c1bac03825d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='im', max=9), IntSlider(value=3, description='slice_by', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict, visualise the transformation of AX train files\n",
    "import numpy as np\n",
    "cfg = config.copy()\n",
    "cfg['BATCHSIZE'] = 10\n",
    "cfg['AUGMENT_GRID'] = False\n",
    "valid_generator = CycleMotionDataGenerator(x_train_ax, x_train_sax, cfg)\n",
    "input_, output_ = valid_generator.__getitem__(0)\n",
    "x_ = input_[0]\n",
    "x2_ = input_[1]\n",
    "y_ = output_[0]\n",
    "y2_ = output_[1]\n",
    "@interact\n",
    "def select_image_in_batch(im = (0,x_.shape[0]- 1, 1), slice_by=(1,6)):\n",
    "    global m\n",
    "    temp = x_[im]\n",
    "    temp_ = y_[im]\n",
    "    sax = x2_[im]\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on:')\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    pred, inv_pred, ax2sax_mod, pred_mask, ax2sax_msk,m_first, m = model.predict(x=[np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])\n",
    "\n",
    "    logging.info('rotated by the model')\n",
    "    show_2D_or_3D(pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('modified rotation of the model')\n",
    "    show_2D_or_3D(ax2sax_mod[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask:')\n",
    "    show_2D_or_3D(pred_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (SAX):')\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverted rotation on SAX')\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(np.reshape(m_first[0],(3,4)))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the heldout test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 20:08:19,977 INFO Create DataGenerator\n",
      "2020-12-03 20:08:19,977 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 42\n",
      " Scaler: MinMax\n",
      " Images: 42 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-03 20:08:19,978 INFO No augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bde9676ae554b3e925284ca81a94a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='im', max=41), IntSlider(value=3, description='slice_by'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = config.copy()\n",
    "cfg['BATCHSIZE'] = len(x_val_ax)\n",
    "v_generator = CycleMotionDataGenerator(x_val_ax, x_val_sax, cfg)\n",
    "input_, output_ = v_generator.__getitem__(0)\n",
    "x_ = input_[0]\n",
    "x2_ = input_[1]\n",
    "y_ = output_[0]\n",
    "y2_ = output_[1]\n",
    "@interact\n",
    "def select_image_in_batch(im = (0,x_.shape[0]- 1, 1), slice_by=(1,6)):\n",
    "    global m\n",
    "    temp = x_[im]\n",
    "    temp_ = y_[im]\n",
    "    sax = x2_[im]\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on:')\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    \n",
    "    pred, inv_pred, ax2sax_mod, pred_mask, ax_mask, m_first, m = model.predict(x=[np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])\n",
    "    logging.info('rotated by the model')\n",
    "    show_2D_or_3D(pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('modified rotation')\n",
    "    show_2D_or_3D(ax2sax_mod[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask')\n",
    "    show_2D_or_3D(pred_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted in AX')\n",
    "    show_2D_or_3D(ax_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (SAX):')\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverted rotation on SAX')\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(np.reshape(m_first[0],(3,4)))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the memory usage\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ax2sax",
   "language": "python",
   "name": "ax2sax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

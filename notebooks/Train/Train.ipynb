{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for root_dir and set working directory\n",
      "Working directory set to: /mnt/ssd/git/3d-mri-domain-adaption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:36:08,117 INFO -------------------- Start --------------------\n",
      "2020-12-07 12:36:08,118 INFO Working directory: /mnt/ssd/git/3d-mri-domain-adaption.\n",
      "2020-12-07 12:36:08,118 INFO Log file: ./logs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data.log\n",
      "2020-12-07 12:36:08,119 INFO Log level for console: INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0', '/gpu:1']\n",
      "{'GPU_IDS': '0,1', 'GPUS': ['/gpu:0', '/gpu:1'], 'ARCHITECTURE': '3D', 'DATASET': 'GCN', 'FOLD': 3, 'EXP_NAME': 'ax_sax/unetwithdownsamplingaugmentation_new_data', 'EXPERIMENT': '3D/ax_sax/unetwithdownsamplingaugmentation_new_data', 'DATA_PATH_AX': '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/', 'DATA_PATH_AX2SAX': '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_to_SAX_3D/', 'DF_PATH': '/mnt/ssd/data/gcn/gcn_05_2020_ax_sax_86/folds.csv', 'MODEL_PATH': 'models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36', 'TENSORBOARD_LOG_DIR': 'reports/tensorboard_logs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36', 'CONFIG_PATH': 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36', 'HISTORY_PATH': 'reports/history/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36', 'DIM': [80, 112, 112], 'DEPTH': 4, 'FILTERS': 16, 'SPACING': [3, 3, 3], 'M_POOL': [2, 2, 2], 'F_SIZE': [3, 3, 3], 'IMG_CHANNELS': 1, 'MASK_VALUES': [1, 2, 3], 'MASK_CLASSES': 3, 'BORDER_MODE': 4, 'IMG_INTERPOLATION': 1, 'MSK_INTERPOLATION': 0, 'AUGMENT': False, 'SHUFFLE': True, 'AUGMENT_GRID': False, 'RESAMPLE': True, 'SCALER': 'MinMax', 'AX_LOSS_WEIGHT': 10.0, 'WEIGHT_MSE_INPLANE': True, 'MASK_SMALLER_THAN_THRESHOLD': 0.001, 'SAX_LOSS_WEIGHT': 10.0, 'CYCLE_LOSS': True, 'FOCUS_LOSS_WEIGHT': 1.0, 'FOCUS_LOSS': True, 'USE_SAX2AX_PROB': False, 'MIN_UNET_PROBABILITY': 0.8, 'GENERATOR_WORKER': 2, 'SEED': 42, 'BATCHSIZE': 2, 'INITIAL_EPOCH': 0, 'EPOCHS': 300, 'EPOCHS_BETWEEN_CHECKPOINTS': 5, 'MONITOR_FUNCTION': 'val_loss', 'MONITOR_MODE': 'min', 'SAVE_MODEL_FUNCTION': 'val_loss', 'SAVE_MODEL_MODE': 'min', 'MODEL_PATIENCE': 20, 'BN_FIRST': False, 'BATCH_NORMALISATION': True, 'USE_UPSAMPLE': True, 'PAD': 'same', 'KERNEL_INIT': 'he_normal', 'OPTIMIZER': 'adam', 'ACTIVATION': 'elu', 'LEARNING_RATE': 0.0001, 'DECAY_FACTOR': 0.3, 'MIN_LR': 1e-10, 'LOSS_FUNCTION': <function bce_dice_loss at 0x7fa723ac5378>}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------define logging and working directory\n",
    "from ProjectRoot import change_wd_to_project_root\n",
    "change_wd_to_project_root()\n",
    "from src.utils.Notebook_imports import *\n",
    "from src.utils.Tensorflow_helper import choose_gpu_by_id\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import cv2\n",
    "# ------------------------------------------define GPU id/s to use\n",
    "GPU_IDS = '0,1'\n",
    "GPUS = choose_gpu_by_id(GPU_IDS)\n",
    "print(GPUS)\n",
    "\n",
    "# ------------------------------------------jupyter magic config\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ------------------------------------------ import helpers\n",
    "from src.utils.Utils_io import Console_and_file_logger, init_config\n",
    "from src.visualization.Visualize import show_2D_or_3D\n",
    "from src.data.Dataset import get_img_msk_files_from_split_dir, load_acdc_files, get_train_data_from_df, get_trainings_files\n",
    "from src.data.Generators import DataGenerator, CycleMotionDataGenerator\n",
    "from src.utils.KerasCallbacks import get_callbacks\n",
    "import src.utils.Loss_and_metrics as metr\n",
    "import src.models.SpatialTransformer as st\n",
    "from src.models.SpatialTransformer import create_affine_cycle_transformer_model\n",
    "from src.models.ModelUtils import load_pretrained_model\n",
    "\n",
    "# ------------------------------------------path and project params\n",
    "ARCHITECTURE = '3D' # 2D\n",
    "DATASET = 'GCN'  # 'acdc' # or 'gcn' or different versions such as gcn_01/02\n",
    "FOLD = 3 # CV fold 0-3\n",
    "EXP_NAME = 'ax_sax/unetwithdownsamplingaugmentation_new_data' # Define an experiment name, could have subfolder conventions\n",
    "EXPERIMENT = '{}/{}'.format(ARCHITECTURE, EXP_NAME) # Uniform path names, separation of concerns\n",
    "timestemp = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")) # ad a timestep to each project to make repeated experiments unique\n",
    "\n",
    "# Our generator expects the following fix data structure (could be changed in src/data/Generators)\n",
    "# any-path/\n",
    "#    - AX_3D(anyname_img.nrrd and anyname_msk.nrrd)\n",
    "#    - AX_to_SAX_3D\n",
    "#    - SAX_3D\n",
    "#    - SAX_to_AX_3D\n",
    "DATA_PATH_AX = '/mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/' # path to AX 3D files\n",
    "DATA_PATH_AX2SAX = DATA_PATH_AX.replace('AX_3D', 'AX_to_SAX_3D') # path to ax2sax 3D files\n",
    "DF_PATH = '/mnt/ssd/data/gcn/gcn_05_2020_ax_sax_86/folds.csv' # path to folds dataframe\n",
    "\n",
    "MODEL_PATH = os.path.join('models', EXPERIMENT, timestemp)\n",
    "TENSORBOARD_LOG_DIR = os.path.join('reports/tensorboard_logs', EXPERIMENT,timestemp)\n",
    "CONFIG_PATH = os.path.join('reports/configs/',EXPERIMENT,timestemp)\n",
    "HISTORY_PATH = os.path.join('reports/history/',EXPERIMENT,timestemp)\n",
    "\n",
    "# ------------------------------------------static model, loss and generator hyperparameters\n",
    "DIM = [80, 112, 112] # network input params for spacing of 3, (z,y,x)\n",
    "DEPTH = 4 # number of down-/upsampling blocks\n",
    "FILTERS = 16 # initial number of filters, will be doubled after each downsampling block\n",
    "SPACING = [3, 3, 3] # if resample, resample to this spacing, (z,y,x)\n",
    "M_POOL = [2, 2, 2]# size of max-pooling used for downsampling and upsampling\n",
    "F_SIZE = [3, 3, 3] # conv filter size\n",
    "IMG_CHANNELS = 1 # Currently our model needs that image channel\n",
    "MASK_VALUES = [1, 2, 3]  #channel order: Background, RV, MYO, LV\n",
    "MASK_CLASSES = len(MASK_VALUES) # no of labels\n",
    "BORDER_MODE = cv2.BORDER_REFLECT_101 # border mode for the data generation\n",
    "IMG_INTERPOLATION = cv2.INTER_LINEAR # image interpolation in the genarator\n",
    "MSK_INTERPOLATION = cv2.INTER_NEAREST # mask interpolation in the generator\n",
    "AUGMENT = False # Not implemented for the AX2SAX case\n",
    "SHUFFLE = True\n",
    "AUGMENT_GRID = False # Not implemented for the AX2SAX case\n",
    "RESAMPLE = True\n",
    "SCALER = 'MinMax' # MinMax Standard or Robust\n",
    "\n",
    "AX_LOSS_WEIGHT = 10.0 # weighting factor of the ax2sax loss\n",
    "WEIGHT_MSE_INPLANE = True # turn inplane weighting on/off\n",
    "MASK_SMALLER_THAN_THRESHOLD = 0.001 # define the threshold for masking the ax2sax/sax2ax MSE loss, areas with smaller values, will be masked out\n",
    "\n",
    "SAX_LOSS_WEIGHT = 10.0 # weighting factor of the sax2ax loss\n",
    "CYCLE_LOSS = True # turn this loss on/off\n",
    "\n",
    "FOCUS_LOSS_WEIGHT = 1.0 # weighting of the focus loss\n",
    "FOCUS_LOSS = True # turn this loss on/off\n",
    "USE_SAX2AX_PROB = False # apply the focus loss on SAX predictions, or on SAX2AX (back-transformed) predictions\n",
    "MIN_UNET_PROBABILITY = 0.8 # threshold to count only prediction greater than this value for the focus loss\n",
    "\n",
    "# ------------------------------------------individual training params\n",
    "GENERATOR_WORKER = 2 # no of parallel workers in our generator. if not set, use batchsize, numbers greater than batchsize does not make any sense\n",
    "SEED = 42 # define a seed for the generator shuffle\n",
    "BATCHSIZE = 2 # 32, 64, 24, 16, 1 for 3spacing 3,3,3 use: 2\n",
    "INITIAL_EPOCH = 0 # change this to continue training\n",
    "EPOCHS = 300 # define a maximum numbers of epochs\n",
    "EPOCHS_BETWEEN_CHECKPOINTS = 5\n",
    "MONITOR_FUNCTION = 'val_loss'\n",
    "MONITOR_MODE = 'min'\n",
    "SAVE_MODEL_FUNCTION = 'val_loss'\n",
    "SAVE_MODEL_MODE = 'min'\n",
    "MODEL_PATIENCE = 20\n",
    "BN_FIRST = False # decide if batch normalisation between conv and activation or afterwards\n",
    "BATCH_NORMALISATION = True # apply BN or not\n",
    "USE_UPSAMPLE = True # otherwise use transpose for upsampling\n",
    "PAD = 'same' # padding strategy of the conv layers\n",
    "KERNEL_INIT = 'he_normal' # conv weight initialisation\n",
    "OPTIMIZER = 'adam' # Adam, Adagrad, RMSprop, Adadelta,  # https://keras.io/optimizers/\n",
    "ACTIVATION = 'elu' # tf.keras.layers.LeakyReLU(), relu or any other non linear activation function\n",
    "LEARNING_RATE = 1e-4 # start with a huge lr to converge fast\n",
    "DECAY_FACTOR = 0.3 # Define a learning rate decay for the ReduceLROnPlateau callback\n",
    "MIN_LR = 1e-10 # minimal lr, smaller lr does not improve the model\n",
    "DROPOUT_min = 0.3 # lower dropout at the shallow layers\n",
    "DROPOUT_max = 0.5 # higher dropout at the deep layers\n",
    "\n",
    "# ------------------------------------------these metrics and loss function are meant if you continue training of the U-Net\n",
    "metrics = [\n",
    "    metr.dice_coef_labels,\n",
    "    metr.dice_coef_myo,\n",
    "    metr.dice_coef_lv,\n",
    "    metr.dice_coef_rv\n",
    "]\n",
    "LOSS_FUNCTION = metr.bce_dice_loss\n",
    "\n",
    "# Create a logger instance with the following setup: info or debug to console and file and error logs to a separate file\n",
    "# Define a config for param injection,\n",
    "# save a serialized version to load the experiment for prediction/evaluation, \n",
    "# make sure all paths exist\n",
    "Console_and_file_logger(EXPERIMENT, logging.INFO)\n",
    "config = init_config(config=locals(), save=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Tensorflow setup and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:36:08,142 INFO Is built with tensorflow: True\n",
      "2020-12-07 12:36:08,220 INFO Visible devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "2020-12-07 12:36:08,896 INFO Local devices: \n",
      " [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10905300224874675564\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8765553060964596435\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17999640948056644406\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3839786896235021600\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23171237120\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16723112076159727072\n",
      "physical_device_desc: \"device: 0, name: TITAN RTX, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23561805568\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3940718526618439944\n",
      "physical_device_desc: \"device: 1, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      "]\n",
      "2020-12-07 12:36:09,070 INFO Compute dtype: float16\n",
      "2020-12-07 12:36:09,070 INFO Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "logging.info('Is built with tensorflow: {}'.format(tf.test.is_built_with_cuda()))\n",
    "logging.info('Visible devices:\\n{}'.format(tf.config.list_physical_devices()))\n",
    "logging.info('Local devices: \\n {}'.format(device_lib.list_local_devices()))\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "logging.info('Compute dtype: %s' % policy.compute_dtype)\n",
    "logging.info('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trainings and validation files for the choosen fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:36:09,110 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/AX_3D/\n",
      "2020-12-07 12:36:09,110 INFO Patients train: 65\n",
      "2020-12-07 12:36:09,116 INFO Selected 122 of 162 files with 65 of 86 patients for training fold 3\n",
      "2020-12-07 12:36:09,116 INFO AX x_train files: 122, AX y_train files: 122\n",
      "2020-12-07 12:36:09,117 INFO AX x_val files: 40, AX y_val files: 40\n",
      "2020-12-07 12:36:09,121 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/AX_to_SAX_3D/\n",
      "2020-12-07 12:36:09,122 INFO Patients train: 65\n",
      "2020-12-07 12:36:09,127 INFO Selected 122 of 162 files with 65 of 86 patients for training fold 3\n",
      "2020-12-07 12:36:09,129 INFO x_train files: 122, y_train files: 122\n",
      "2020-12-07 12:36:09,130 INFO x_val files: 40, y_val files: 40\n"
     ]
    }
   ],
   "source": [
    "# Load AX volumes\n",
    "x_train_ax, y_train_ax, x_val_ax, y_val_ax =  get_trainings_files(data_path=DATA_PATH_AX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "\n",
    "logging.info('AX x_train files: {}, AX y_train files: {}'.format(len(x_train_ax), len(y_train_ax)))\n",
    "logging.info('AX x_val files: {}, AX y_val files: {}'.format(len(x_val_ax), len(y_val_ax)))\n",
    "\n",
    "# load AX2SAX volumes, they should be in the same directory but with a different suffix --> AX_to_SAX_3D\n",
    "x_train_sax, y_train_sax, x_val_sax, y_val_sax =  get_trainings_files(data_path=DATA_PATH_AX2SAX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "config = init_config(config)\n",
    "\n",
    "logging.info('x_train files: {}, y_train files: {}'.format(len(x_train_sax), len(y_train_sax)))\n",
    "logging.info('x_val files: {}, y_val files: {}'.format(len(x_val_sax), len(y_val_sax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# filter files by name, debugging purpose\n",
    "#x_val_ax = [x for x in x_val_ax if '4A4PVCYL_2006' in x]\n",
    "#x_val_sax = [x for x in x_val_sax if '4A4PVCYL_2006' in x]\n",
    "#y_val_ax = [x for x in y_val_ax if '4A4PVCYL_2006' in x]\n",
    "print(len(x_val_ax))\n",
    "print(len(x_val_sax))\n",
    "print(len(y_val_ax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:36:09,849 INFO Create DataGenerator\n",
      "2020-12-07 12:36:09,850 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 2\n",
      " Scaler: MinMax\n",
      " Images: 122 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-07 12:36:09,852 INFO No augmentation\n",
      "2020-12-07 12:36:09,853 INFO Create DataGenerator\n",
      "2020-12-07 12:36:09,853 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 2\n",
      " Scaler: MinMax\n",
      " Images: 40 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-07 12:36:09,854 INFO No augmentation\n"
     ]
    }
   ],
   "source": [
    "# create two generators, one for the training files, one for the validation files\n",
    "batch_generator = CycleMotionDataGenerator(x_train_ax, x_train_sax, config)\n",
    "valid_config = config.copy()\n",
    "valid_config['AUGMENT_GRID'] = False\n",
    "valid_config['AUGMENT'] = False\n",
    "valid_generator = CycleMotionDataGenerator(x_val_ax, x_val_sax, valid_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd2c645a85243cea118d0cec887b6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='batch', max=20), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select batch generator output\n",
    "x = ''\n",
    "y = ''\n",
    "@interact\n",
    "def select_batch(batch = (0,len(valid_generator), 1)):\n",
    "    global x, y, x2, y2\n",
    "    input_ , output_ = valid_generator.__getitem__(batch)\n",
    "    x = input_[0]\n",
    "    y = output_[0]\n",
    "    x2 = input_[1]\n",
    "    y2 = output_[1]\n",
    "    logging.info('input elements: {}'.format(len(input_)))\n",
    "    logging.info('output elements: {}'.format(len(output_)))\n",
    "    logging.info(x.shape)\n",
    "    logging.info(y.shape)\n",
    "    logging.info(x2.shape)\n",
    "    logging.info(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e725a329fc9448e9bdcf3e70642d452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='im', max=1), IntSlider(value=3, description='slice_by', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def select_image_in_batch(im = (0,x.shape[0]- 1, 1),slice_by=(1,6)):\n",
    "    \n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.DEBUG)\n",
    "    temp_dir = 'reports/figures/temp/'\n",
    "    ensure_dir(temp_dir)\n",
    "\n",
    "    logging.info('AX: {}'.format(x[im].shape))\n",
    "    show_2D_or_3D(x[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'ax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('AXtoSAX: {}'.format(y[im].shape))\n",
    "    show_2D_or_3D(y[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'ax2sax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('SAX: {}'.format(x2[im].shape))\n",
    "    show_2D_or_3D(x2[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'sax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('SAXtoAX: {}'.format(y2[im].shape))\n",
    "    show_2D_or_3D(y2[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'sax2ax.pdf'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:36:12,454 INFO Load model from Experiment: 2D/gcn_05_2020_sax_excl_ax_patients\n",
      "2020-12-07 12:36:12,454 INFO load model with keras api\n",
      "2020-12-07 12:36:14,820 INFO Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "2020-12-07 12:36:14,821 INFO Keras API failed, use json repr. load model from: models/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/model.json .\n",
      "2020-12-07 12:36:14,821 INFO loading model description\n",
      "2020-12-07 12:36:15,687 INFO loading model weights\n",
      "2020-12-07 12:36:15,859 INFO model models/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/model.json loaded\n"
     ]
    }
   ],
   "source": [
    "# load a pretrained 2D unet\n",
    "\"\"\"\n",
    "load past config for model training \n",
    "\"\"\"\n",
    "if 'config_chooser' in locals():\n",
    "    config_file  = config_chooser.selected\n",
    "else:\n",
    "    #config_file = '/mnt/ssd/git/3d-mri-domain-adaption/reports/configs/2D/gcn_and_acdc_excl_ax/config.json' # config for TMI paper\n",
    "    config_file = '/mnt/ssd/git/cardio/reports/configs/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/config.json' # retrained with downsampling\n",
    "\n",
    "# load config with all params into global namespace\n",
    "with open(config_file, encoding='utf-8') as data_file:\n",
    "    config_temp = json.loads(data_file.read())\n",
    "logging.info('Load model from Experiment: {}'.format(config_temp['EXPERIMENT']))\n",
    "\n",
    "if 'strategy' not in globals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "with strategy.scope():\n",
    "    globals()['unet'] = load_pretrained_model(config_temp, metrics, comp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:36:18,262 INFO unet given, use it to max probability\n",
      "2020-12-07 12:36:36,886 INFO adding ax2sax MSE loss with a weighting of 10.0\n",
      "2020-12-07 12:36:36,887 INFO adding cycle loss with a weighting of 10.0\n",
      "2020-12-07 12:36:36,887 INFO adding focus loss on mask_prob with a weighting of 1.0\n"
     ]
    }
   ],
   "source": [
    "if 'strategy' not in globals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "# inject the pre-trained unet if given, otherwise build the model without the pretrained unet\n",
    "with strategy.scope():\n",
    "    model = st.create_affine_cycle_transformer_model(config=config, metrics=metrics, unet=locals().get('unet', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"affine_cycle_transformer\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             [(None, 80, 112, 112, 1)]        0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv_encoder (ConvEncoder)                       ((None, 5, 7, 7, 256), [(None, 8 3537424           input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "global_average_pooling3d (GlobalAveragePooling3D (None, 256)                      0                 conv_encoder[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense1 (Dense)                                   (None, 256)                      65792             global_average_pooling3d[0][0]                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense2 (Dense)                                   (None, 9)                        2313              dense1[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (TensorFlowOpLayer)  [(None, 3)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (TensorFlowOpLayer)  [(None, 3)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                        (None, 6)                        0                 tf_op_layer_strided_slice_1[0][0]                 \n",
      "                                                                                                    tf_op_layer_strided_slice_2[0][0]                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (TensorFlowOpLayer)    [(None, 6)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_mod_matrix (Euler2Matrix)                 (None, 12)                       0                 concatenate[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_matrix (Euler2Matrix)                     (None, 12)                       0                 tf_op_layer_strided_slice[0][0]                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_mod_st (SpatialTransformer)               (None, 80, 112, 112, 1)          0                 input_1[0][0]                                     \n",
      "                                                                                                    ax2sax_mod_matrix[0][0]                           \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                             [(None, 80, 112, 112, 1)]        0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "inverse3d_matrix (Inverse3DMatrix)               (None, 12)                       0                 ax2sax_matrix[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "mask_prob (UnetWrapper)                          (None, 80, 112, 112, 3)          19432275          ax2sax_mod_st[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "inverse3d_matrix_1 (Inverse3DMatrix)             (None, 12)                       0                 ax2sax_mod_matrix[0][0]                           \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax (SpatialTransformer)                      (None, 80, 112, 112, 1)          0                 input_1[0][0]                                     \n",
      "                                                                                                    ax2sax_matrix[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "sax2ax (SpatialTransformer)                      (None, 80, 112, 112, 1)          0                 input_2[0][0]                                     \n",
      "                                                                                                    inverse3d_matrix[0][0]                            \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "mask2ax (SpatialTransformer)                     (None, 80, 112, 112, 3)          0                 mask_prob[0][0]                                   \n",
      "                                                                                                    inverse3d_matrix_1[0][0]                          \n",
      "======================================================================================================================================================\n",
      "Total params: 23,037,804\n",
      "Trainable params: 3,603,545\n",
      "Non-trainable params: 19,434,259\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=150)\n",
    "#plot_model(model, to_file='reports/figures/temp_graph.pdf',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21431215a7bb4adbb50746a9e58639db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='im', max=1), Text(value='0.001', description='mask_small…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def select_image_in_batch(im = (0,x.shape[0]- 1, 1),mask_smaller_than='0.001', slice_by=(1,6)):\n",
    "    global m\n",
    "    import numpy as np\n",
    "    temp = x[im]\n",
    "    sax = x2[im]\n",
    "    temp_ = y[im]\n",
    "    \n",
    "    mask = temp_ >float(mask_smaller_than)\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on: {}'.format(temp.shape))\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    pred, inv_pred, ax2sax_mod, prob, ax_msk,m, m_mod = model.predict(x = [np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])                     \n",
    "    logging.info('rotated by the model: {}'.format(pred[0].shape))\n",
    "    show_2D_or_3D(pred[0][::slice_by], mask[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverse rotation on SAX: {}'.format(inv_pred[0].shape))\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask: {}'.format(inv_pred[0].shape))\n",
    "    show_2D_or_3D(prob[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask in ax: {}'.format(ax_msk[0].shape))\n",
    "    show_2D_or_3D(ax_msk[0][::slice_by])\n",
    "    plt.show()\n",
    "    \n",
    "    # calculate the loss mask from target AX2SAX image\n",
    "    mask = temp_ >float(mask_smaller_than)\n",
    "    logging.info('masked by GT: {}'.format(mask.shape))\n",
    "    masked = pred[0] * mask\n",
    "    show_2D_or_3D(masked[::slice_by], mask[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (AX2SAX): {}'.format(temp_.shape))\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('Created MSE mask by thresholding the target (AX2SAX) with {}: {}'.format(mask_smaller_than,temp_.shape))\n",
    "    show_2D_or_3D(mask[::slice_by])\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        from tensorflow.keras.metrics import MSE as mse\n",
    "        logging.info('MSE: {}'.format(mse(pred[0], temp_).numpy().mean()))\n",
    "        logging.info('prob loss: {}'.format(metr.max_volume_loss(min_probabillity=0.5)(temp_[tf.newaxis,...],prob).numpy().mean()))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "        print(np.reshape(m_mod[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 12:37:29,306 INFO Fit model, start trainings process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 21.3162 - ax2sax_loss: 1.1262 - sax2ax_loss: 0.9062 - mask_prob_loss: 0.9924\n",
      "Epoch 00001: val_loss improved from inf to 20.35625, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 165s 3s/step - loss: 21.3162 - ax2sax_loss: 1.1262 - sax2ax_loss: 0.9062 - mask_prob_loss: 0.9924 - val_loss: 20.3563 - val_ax2sax_loss: 1.0965 - val_sax2ax_loss: 0.8399 - val_mask_prob_loss: 0.9923 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 20.8820 - ax2sax_loss: 1.0988 - sax2ax_loss: 0.8901 - mask_prob_loss: 0.9929\n",
      "Epoch 00002: val_loss improved from 20.35625 to 20.01574, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 123s 2s/step - loss: 20.8820 - ax2sax_loss: 1.0988 - sax2ax_loss: 0.8901 - mask_prob_loss: 0.9929 - val_loss: 20.0157 - val_ax2sax_loss: 1.0844 - val_sax2ax_loss: 0.8178 - val_mask_prob_loss: 0.9935 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 20.7247 - ax2sax_loss: 1.0918 - sax2ax_loss: 0.8813 - mask_prob_loss: 0.9936\n",
      "Epoch 00003: val_loss improved from 20.01574 to 19.55305, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 127s 2s/step - loss: 20.7247 - ax2sax_loss: 1.0918 - sax2ax_loss: 0.8813 - mask_prob_loss: 0.9936 - val_loss: 19.5530 - val_ax2sax_loss: 1.0617 - val_sax2ax_loss: 0.7939 - val_mask_prob_loss: 0.9968 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 20.1539 - ax2sax_loss: 1.0659 - sax2ax_loss: 0.8500 - mask_prob_loss: 0.9954\n",
      "Epoch 00004: val_loss improved from 19.55305 to 19.40680, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 127s 2s/step - loss: 20.1539 - ax2sax_loss: 1.0659 - sax2ax_loss: 0.8500 - mask_prob_loss: 0.9954 - val_loss: 19.4068 - val_ax2sax_loss: 1.0511 - val_sax2ax_loss: 0.7899 - val_mask_prob_loss: 0.9961 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.9400 - ax2sax_loss: 0.9922 - sax2ax_loss: 0.8024 - mask_prob_loss: 0.9943\n",
      "Epoch 00005: val_loss did not improve from 19.40680\n",
      "61/61 [==============================] - 130s 2s/step - loss: 18.9400 - ax2sax_loss: 0.9922 - sax2ax_loss: 0.8024 - mask_prob_loss: 0.9943 - val_loss: 19.8005 - val_ax2sax_loss: 1.0445 - val_sax2ax_loss: 0.8364 - val_mask_prob_loss: 0.9914 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 18.2779 - ax2sax_loss: 0.9453 - sax2ax_loss: 0.7834 - mask_prob_loss: 0.9906\n",
      "Epoch 00006: val_loss improved from 19.40680 to 19.14682, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 129s 2s/step - loss: 18.2779 - ax2sax_loss: 0.9453 - sax2ax_loss: 0.7834 - mask_prob_loss: 0.9906 - val_loss: 19.1468 - val_ax2sax_loss: 0.9933 - val_sax2ax_loss: 0.8224 - val_mask_prob_loss: 0.9896 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 17.7857 - ax2sax_loss: 0.9199 - sax2ax_loss: 0.7597 - mask_prob_loss: 0.9899\n",
      "Epoch 00007: val_loss improved from 19.14682 to 19.11719, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 132s 2s/step - loss: 17.7857 - ax2sax_loss: 0.9199 - sax2ax_loss: 0.7597 - mask_prob_loss: 0.9899 - val_loss: 19.1172 - val_ax2sax_loss: 0.9914 - val_sax2ax_loss: 0.8214 - val_mask_prob_loss: 0.9891 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 17.2782 - ax2sax_loss: 0.8940 - sax2ax_loss: 0.7349 - mask_prob_loss: 0.9897\n",
      "Epoch 00008: val_loss improved from 19.11719 to 18.44006, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 130s 2s/step - loss: 17.2782 - ax2sax_loss: 0.8940 - sax2ax_loss: 0.7349 - mask_prob_loss: 0.9897 - val_loss: 18.4401 - val_ax2sax_loss: 0.9490 - val_sax2ax_loss: 0.7961 - val_mask_prob_loss: 0.9895 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 16.6640 - ax2sax_loss: 0.8609 - sax2ax_loss: 0.7065 - mask_prob_loss: 0.9896\n",
      "Epoch 00009: val_loss improved from 18.44006 to 18.01692, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 129s 2s/step - loss: 16.6640 - ax2sax_loss: 0.8609 - sax2ax_loss: 0.7065 - mask_prob_loss: 0.9896 - val_loss: 18.0169 - val_ax2sax_loss: 0.9250 - val_sax2ax_loss: 0.7777 - val_mask_prob_loss: 0.9899 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 16.0674 - ax2sax_loss: 0.8251 - sax2ax_loss: 0.6827 - mask_prob_loss: 0.9897\n",
      "Epoch 00010: val_loss improved from 18.01692 to 17.42826, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 128s 2s/step - loss: 16.0674 - ax2sax_loss: 0.8251 - sax2ax_loss: 0.6827 - mask_prob_loss: 0.9897 - val_loss: 17.4283 - val_ax2sax_loss: 0.8925 - val_sax2ax_loss: 0.7514 - val_mask_prob_loss: 0.9900 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.6300 - ax2sax_loss: 0.7993 - sax2ax_loss: 0.6647 - mask_prob_loss: 0.9898\n",
      "Epoch 00011: val_loss improved from 17.42826 to 17.03497, saving model to models/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/model.h5\n",
      "61/61 [==============================] - 130s 2s/step - loss: 15.6300 - ax2sax_loss: 0.7993 - sax2ax_loss: 0.6647 - mask_prob_loss: 0.9898 - val_loss: 17.0350 - val_ax2sax_loss: 0.8715 - val_sax2ax_loss: 0.7330 - val_mask_prob_loss: 0.9899 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.4555 - ax2sax_loss: 0.7874 - sax2ax_loss: 0.6591 - mask_prob_loss: 0.9899\n",
      "Epoch 00012: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 128s 2s/step - loss: 15.4555 - ax2sax_loss: 0.7874 - sax2ax_loss: 0.6591 - mask_prob_loss: 0.9899 - val_loss: 17.5674 - val_ax2sax_loss: 0.8949 - val_sax2ax_loss: 0.7628 - val_mask_prob_loss: 0.9903 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.3089 - ax2sax_loss: 0.7806 - sax2ax_loss: 0.6513 - mask_prob_loss: 0.9900\n",
      "Epoch 00013: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 130s 2s/step - loss: 15.3089 - ax2sax_loss: 0.7806 - sax2ax_loss: 0.6513 - mask_prob_loss: 0.9900 - val_loss: 17.4522 - val_ax2sax_loss: 0.8884 - val_sax2ax_loss: 0.7578 - val_mask_prob_loss: 0.9903 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.3268 - ax2sax_loss: 0.7808 - sax2ax_loss: 0.6528 - mask_prob_loss: 0.9900\n",
      "Epoch 00014: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 128s 2s/step - loss: 15.3268 - ax2sax_loss: 0.7808 - sax2ax_loss: 0.6528 - mask_prob_loss: 0.9900 - val_loss: 17.3531 - val_ax2sax_loss: 0.8830 - val_sax2ax_loss: 0.7533 - val_mask_prob_loss: 0.9902 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.3720 - ax2sax_loss: 0.7830 - sax2ax_loss: 0.6552 - mask_prob_loss: 0.9901\n",
      "Epoch 00015: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 129s 2s/step - loss: 15.3720 - ax2sax_loss: 0.7830 - sax2ax_loss: 0.6552 - mask_prob_loss: 0.9901 - val_loss: 17.4151 - val_ax2sax_loss: 0.8855 - val_sax2ax_loss: 0.7570 - val_mask_prob_loss: 0.9903 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.3048 - ax2sax_loss: 0.7789 - sax2ax_loss: 0.6526 - mask_prob_loss: 0.9901\n",
      "Epoch 00016: val_loss did not improve from 17.03497\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "61/61 [==============================] - 130s 2s/step - loss: 15.3048 - ax2sax_loss: 0.7789 - sax2ax_loss: 0.6526 - mask_prob_loss: 0.9901 - val_loss: 17.4398 - val_ax2sax_loss: 0.8868 - val_sax2ax_loss: 0.7582 - val_mask_prob_loss: 0.9904 - lr: 3.0000e-05\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2972 - ax2sax_loss: 0.7795 - sax2ax_loss: 0.6512 - mask_prob_loss: 0.9901\n",
      "Epoch 00017: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 132s 2s/step - loss: 15.2972 - ax2sax_loss: 0.7795 - sax2ax_loss: 0.6512 - mask_prob_loss: 0.9901 - val_loss: 17.3848 - val_ax2sax_loss: 0.8845 - val_sax2ax_loss: 0.7549 - val_mask_prob_loss: 0.9903 - lr: 3.0000e-05\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2781 - ax2sax_loss: 0.7785 - sax2ax_loss: 0.6503 - mask_prob_loss: 0.9901\n",
      "Epoch 00018: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 132s 2s/step - loss: 15.2781 - ax2sax_loss: 0.7785 - sax2ax_loss: 0.6503 - mask_prob_loss: 0.9901 - val_loss: 17.4011 - val_ax2sax_loss: 0.8854 - val_sax2ax_loss: 0.7557 - val_mask_prob_loss: 0.9904 - lr: 3.0000e-05\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2742 - ax2sax_loss: 0.7788 - sax2ax_loss: 0.6496 - mask_prob_loss: 0.9901\n",
      "Epoch 00019: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 131s 2s/step - loss: 15.2742 - ax2sax_loss: 0.7788 - sax2ax_loss: 0.6496 - mask_prob_loss: 0.9901 - val_loss: 17.3477 - val_ax2sax_loss: 0.8829 - val_sax2ax_loss: 0.7529 - val_mask_prob_loss: 0.9903 - lr: 3.0000e-05\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2732 - ax2sax_loss: 0.7783 - sax2ax_loss: 0.6500 - mask_prob_loss: 0.9901\n",
      "Epoch 00020: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 129s 2s/step - loss: 15.2732 - ax2sax_loss: 0.7783 - sax2ax_loss: 0.6500 - mask_prob_loss: 0.9901 - val_loss: 17.3615 - val_ax2sax_loss: 0.8836 - val_sax2ax_loss: 0.7535 - val_mask_prob_loss: 0.9904 - lr: 3.0000e-05\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2737 - ax2sax_loss: 0.7782 - sax2ax_loss: 0.6501 - mask_prob_loss: 0.9901\n",
      "Epoch 00021: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 129s 2s/step - loss: 15.2737 - ax2sax_loss: 0.7782 - sax2ax_loss: 0.6501 - mask_prob_loss: 0.9901 - val_loss: 17.3315 - val_ax2sax_loss: 0.8822 - val_sax2ax_loss: 0.7519 - val_mask_prob_loss: 0.9903 - lr: 3.0000e-05\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2702 - ax2sax_loss: 0.7780 - sax2ax_loss: 0.6500 - mask_prob_loss: 0.9901\n",
      "Epoch 00022: val_loss did not improve from 17.03497\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "61/61 [==============================] - 128s 2s/step - loss: 15.2702 - ax2sax_loss: 0.7780 - sax2ax_loss: 0.6500 - mask_prob_loss: 0.9901 - val_loss: 17.3374 - val_ax2sax_loss: 0.8823 - val_sax2ax_loss: 0.7525 - val_mask_prob_loss: 0.9903 - lr: 9.0000e-06\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2677 - ax2sax_loss: 0.7779 - sax2ax_loss: 0.6499 - mask_prob_loss: 0.9901\n",
      "Epoch 00023: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 132s 2s/step - loss: 15.2677 - ax2sax_loss: 0.7779 - sax2ax_loss: 0.6499 - mask_prob_loss: 0.9901 - val_loss: 17.3832 - val_ax2sax_loss: 0.8843 - val_sax2ax_loss: 0.7550 - val_mask_prob_loss: 0.9904 - lr: 9.0000e-06\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2672 - ax2sax_loss: 0.7778 - sax2ax_loss: 0.6499 - mask_prob_loss: 0.9901\n",
      "Epoch 00024: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 131s 2s/step - loss: 15.2672 - ax2sax_loss: 0.7778 - sax2ax_loss: 0.6499 - mask_prob_loss: 0.9901 - val_loss: 17.4339 - val_ax2sax_loss: 0.8864 - val_sax2ax_loss: 0.7579 - val_mask_prob_loss: 0.9904 - lr: 9.0000e-06\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2674 - ax2sax_loss: 0.7779 - sax2ax_loss: 0.6498 - mask_prob_loss: 0.9901\n",
      "Epoch 00025: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 129s 2s/step - loss: 15.2674 - ax2sax_loss: 0.7779 - sax2ax_loss: 0.6498 - mask_prob_loss: 0.9901 - val_loss: 17.4021 - val_ax2sax_loss: 0.8852 - val_sax2ax_loss: 0.7560 - val_mask_prob_loss: 0.9905 - lr: 9.0000e-06\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2662 - ax2sax_loss: 0.7778 - sax2ax_loss: 0.6498 - mask_prob_loss: 0.9901\n",
      "Epoch 00026: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 128s 2s/step - loss: 15.2662 - ax2sax_loss: 0.7778 - sax2ax_loss: 0.6498 - mask_prob_loss: 0.9901 - val_loss: 17.4483 - val_ax2sax_loss: 0.8871 - val_sax2ax_loss: 0.7587 - val_mask_prob_loss: 0.9905 - lr: 9.0000e-06\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2631 - ax2sax_loss: 0.7778 - sax2ax_loss: 0.6495 - mask_prob_loss: 0.9901\n",
      "Epoch 00027: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 130s 2s/step - loss: 15.2631 - ax2sax_loss: 0.7778 - sax2ax_loss: 0.6495 - mask_prob_loss: 0.9901 - val_loss: 17.4594 - val_ax2sax_loss: 0.8876 - val_sax2ax_loss: 0.7593 - val_mask_prob_loss: 0.9905 - lr: 9.0000e-06\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2671 - ax2sax_loss: 0.7779 - sax2ax_loss: 0.6498 - mask_prob_loss: 0.9901\n",
      "Epoch 00028: val_loss did not improve from 17.03497\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
      "61/61 [==============================] - 129s 2s/step - loss: 15.2671 - ax2sax_loss: 0.7779 - sax2ax_loss: 0.6498 - mask_prob_loss: 0.9901 - val_loss: 17.4748 - val_ax2sax_loss: 0.8882 - val_sax2ax_loss: 0.7603 - val_mask_prob_loss: 0.9905 - lr: 2.7000e-06\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2610 - ax2sax_loss: 0.7777 - sax2ax_loss: 0.6494 - mask_prob_loss: 0.9901\n",
      "Epoch 00029: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 129s 2s/step - loss: 15.2610 - ax2sax_loss: 0.7777 - sax2ax_loss: 0.6494 - mask_prob_loss: 0.9901 - val_loss: 17.4580 - val_ax2sax_loss: 0.8873 - val_sax2ax_loss: 0.7594 - val_mask_prob_loss: 0.9905 - lr: 2.7000e-06\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2614 - ax2sax_loss: 0.7777 - sax2ax_loss: 0.6494 - mask_prob_loss: 0.9901\n",
      "Epoch 00030: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 127s 2s/step - loss: 15.2614 - ax2sax_loss: 0.7777 - sax2ax_loss: 0.6494 - mask_prob_loss: 0.9901 - val_loss: 17.4196 - val_ax2sax_loss: 0.8858 - val_sax2ax_loss: 0.7571 - val_mask_prob_loss: 0.9904 - lr: 2.7000e-06\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - ETA: 0s - loss: 15.2620 - ax2sax_loss: 0.7777 - sax2ax_loss: 0.6495 - mask_prob_loss: 0.9901\n",
      "Epoch 00031: val_loss did not improve from 17.03497\n",
      "61/61 [==============================] - 131s 2s/step - loss: 15.2620 - ax2sax_loss: 0.7777 - sax2ax_loss: 0.6495 - mask_prob_loss: 0.9901 - val_loss: 17.4336 - val_ax2sax_loss: 0.8864 - val_sax2ax_loss: 0.7579 - val_mask_prob_loss: 0.9905 - lr: 2.7000e-06\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "# train one model\n",
    "initial_epoch = 0\n",
    "logging.info('Fit model, start trainings process')\n",
    "# fit model with trainingsgenerator\n",
    "results = model.fit(\n",
    "    x=batch_generator,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=len(valid_generator),\n",
    "    epochs=200,\n",
    "    callbacks = get_callbacks(config, valid_generator),\n",
    "    steps_per_epoch = len(batch_generator),\n",
    "    initial_epoch=initial_epoch,\n",
    "    max_queue_size=20,\n",
    "    workers=8,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if for any reason you want to save the latest model, use this cell\n",
    "#tf.keras.models.save_model(model,filepath=config['MODEL_PATH'],overwrite=True,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['MODEL_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 14:30:29,323 INFO Load model from Experiment: 3D/ax_sax/unetwithdownsamplingaugmentation_new_data\n",
      "2020-12-07 14:30:30,068 INFO unet given, use it to max probability\n",
      "2020-12-07 14:30:48,204 INFO adding ax2sax MSE loss with a weighting of 10.0\n",
      "2020-12-07 14:30:48,205 INFO adding cycle loss with a weighting of 10.0\n",
      "2020-12-07 14:30:48,206 INFO adding focus loss on mask_prob with a weighting of 1.0\n",
      "2020-12-07 14:30:48,484 INFO loaded model weights as h5 file\n"
     ]
    }
   ],
   "source": [
    "# Fast tests of a trained model, the \"real\" predictions will be done in src/notebooks/Predict\n",
    "\n",
    "\"\"\"\n",
    "load past config for model training \n",
    "\"\"\"\n",
    "if 'strategy' not in locals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "\n",
    "# round the crop and pad values instead of ceil\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-03_18_20/config.json' # Fold 0\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-03_22_02/config.json' # Fold 1\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-04_16_56/config.json' # Fold 2\n",
    "config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/config.json' # Fold 3\n",
    "\n",
    "\n",
    "# load a pre-trained ax2sax model, create the graph and load the weights separately, due to own loss functions, this is easier\n",
    "with open(config_file, encoding='utf-8') as data_file:\n",
    "    config_temp = json.loads(data_file.read())\n",
    "config_temp['LOSS_FUNCTION'] = config['LOSS_FUNCTION']\n",
    "logging.info('Load model from Experiment: {}'.format(config_temp['EXPERIMENT']))\n",
    "\n",
    "with strategy.scope():\n",
    "    globals()['model'] = st.create_affine_cycle_transformer_model(config=config_temp, metrics=metrics, unet=locals().get('unet', None))\n",
    "    model.load_weights(os.path.join(config_temp['MODEL_PATH'],'model.h5'))\n",
    "    logging.info('loaded model weights as h5 file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast predictions with all files of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 20:07:27,707 INFO Create DataGenerator\n",
      "2020-12-03 20:07:27,708 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 10\n",
      " Scaler: MinMax\n",
      " Images: 120 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-03 20:07:27,709 INFO No augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcda926c12e41d386c1bac03825d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='im', max=9), IntSlider(value=3, description='slice_by', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict, visualise the transformation of AX train files\n",
    "import numpy as np\n",
    "cfg = config.copy()\n",
    "cfg['BATCHSIZE'] = 10\n",
    "cfg['AUGMENT_GRID'] = False\n",
    "valid_generator = CycleMotionDataGenerator(x_train_ax, x_train_sax, cfg)\n",
    "input_, output_ = valid_generator.__getitem__(0)\n",
    "x_ = input_[0]\n",
    "x2_ = input_[1]\n",
    "y_ = output_[0]\n",
    "y2_ = output_[1]\n",
    "@interact\n",
    "def select_image_in_batch(im = (0,x_.shape[0]- 1, 1), slice_by=(1,6)):\n",
    "    global m\n",
    "    temp = x_[im]\n",
    "    temp_ = y_[im]\n",
    "    sax = x2_[im]\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on:')\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    pred, inv_pred, ax2sax_mod, pred_mask, ax2sax_msk,m_first, m = model.predict(x=[np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])\n",
    "\n",
    "    logging.info('rotated by the model')\n",
    "    show_2D_or_3D(pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('modified rotation of the model')\n",
    "    show_2D_or_3D(ax2sax_mod[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask:')\n",
    "    show_2D_or_3D(pred_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (SAX):')\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverted rotation on SAX')\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(np.reshape(m_first[0],(3,4)))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the heldout test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 20:08:19,977 INFO Create DataGenerator\n",
      "2020-12-03 20:08:19,977 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 42\n",
      " Scaler: MinMax\n",
      " Images: 42 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-03 20:08:19,978 INFO No augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bde9676ae554b3e925284ca81a94a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='im', max=41), IntSlider(value=3, description='slice_by'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = config.copy()\n",
    "cfg['BATCHSIZE'] = len(x_val_ax)\n",
    "v_generator = CycleMotionDataGenerator(x_val_ax, x_val_sax, cfg)\n",
    "input_, output_ = v_generator.__getitem__(0)\n",
    "x_ = input_[0]\n",
    "x2_ = input_[1]\n",
    "y_ = output_[0]\n",
    "y2_ = output_[1]\n",
    "@interact\n",
    "def select_image_in_batch(im = (0,x_.shape[0]- 1, 1), slice_by=(1,6)):\n",
    "    global m\n",
    "    temp = x_[im]\n",
    "    temp_ = y_[im]\n",
    "    sax = x2_[im]\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on:')\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    \n",
    "    pred, inv_pred, ax2sax_mod, pred_mask, ax_mask, m_first, m = model.predict(x=[np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])\n",
    "    logging.info('rotated by the model')\n",
    "    show_2D_or_3D(pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('modified rotation')\n",
    "    show_2D_or_3D(ax2sax_mod[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask')\n",
    "    show_2D_or_3D(pred_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted in AX')\n",
    "    show_2D_or_3D(ax_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (SAX):')\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverted rotation on SAX')\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(np.reshape(m_first[0],(3,4)))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the memory usage\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ax2sax",
   "language": "python",
   "name": "ax2sax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
